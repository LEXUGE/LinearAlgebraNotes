% Created 2020-06-11 Thu 22:59
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue}
\usepackage[margin=0.5in]{geometry}
\author{Harry Ying}
\date{}
\title{Linear Algebra Notes}
\hypersetup{
 pdfauthor={Harry Ying},
 pdftitle={Linear Algebra Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)},
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents \clearpage
\section{Chapter 1}
\label{sec:orgc6a9dae}
\subsection{\(\S\) 1}
\label{sec:org6fe19ee}
\subsubsection{Notes}
\label{sec:orgd833d61}
A generic vector space \(V\) is not a field because there is no definition of \(v^{-1}\) for some \(v\in V\), fulfilling not the definition of a field.\\
\begin{enumerate}
\item \textbf{Pg. 4 Proof of \((-1)v=v\)}
\label{sec:orga3f06dc}
$$\begin{aligned}
(-1)v+v&=(-1)v+1\cdot v\\
&=(-1+1)v\\
&=v+(-v)
\end{aligned}$$
Thus, \((-1)v=-v\).
\item \textbf{Pg. 6 Proof of SP 3}
\label{sec:org3e6b498}
$$\begin{aligned}
(xA)\cdot B&=\sum\limits_{i=1}^{n}(xa_i) b_{i}\\
&=\sum\limits_{i=1}^{n}x(a_i b_{i})\\
&=x\sum\limits_{i=1}^{n} a_i b_{i}\\
&=x(A\cdot B)\\
A\cdot (xB)&=\sum\limits_{i=1}^{n}a_i (xb_{i})\\
&=\sum\limits_{i=1}^{n}x(a_i b_{i})\\
&=x\sum\limits_{i=1}^{n} a_i b_{i}\\
&=x(A\cdot B)
\end{aligned}$$
\item \textbf{\textbf{Pg. 7}}
\label{sec:org88c1660}

\label{org5da29d8}
Upper one:
$$\begin{aligned}
(A+B)^2&=(A+B)\cdot (A+B)\\
&=(A+B)\cdot A+(A+B)\cdot B && \text{Use SP 2}\\
&=A^2+B\cdot A+A\cdot B+B^2 && \text{Use SP 1}\\
\end{aligned}$$
Bottom one:
Since \(K\) is a field, all \textbf{VS} s regarding summation or product of functions are actually closed on \(K\). By applying field axioms, \(V\) is then a vector space over \(K\).
\item \textbf{\textbf{Pg. 9}}
\label{sec:org978dc55}

\label{org6f18be9}
Let \(a_1=(u_1+w_1),a_2=(u_2+w_2)\). Both of them \(\in (U+W)\).\\
Since \(U,W\) are subspaces of \(V\), \(U,W\in V\). Thus, \(a_1,a_2 \in V\) as \(u_1,w_1,u_2,w_2\in V\), moreover, \((U+W)\subset V\).\\
\(a_1+a_2=(u_1+u_2)+(w_1+w_2)\in (U+W)\) \\
\(ca_1=c(u_1+w_1)=(cu_1)+(cw_1)\in (U+W)\) \\
Since \(O\in U\) and \(O\in W\), \(O=O+O\in (U+W)\). Thus, \((U+W)\) is a subspace of \(V\).
\end{enumerate}
\subsubsection{Exercises}
\label{sec:org963f33c}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:org91a462b}
Let \(v\in{} V\), \(c[v+(-v)]=cv+c(-v)=cv+(-c)v=v\cdot{}0=v\cdot{}(1-1)=v+(-v)=O\)
\item \textbf{Exercise 2}
\label{sec:orgaecccd5}
Since \(c\not = 0\)
$$\begin{aligned}
O&=cv+[-(cv)]\\
cv&=cv+[-(cv)]\\
O&=-(cv)\\
\frac{-1}{c}\cdot O &= (-c)v\cdot \frac{-1}{c}\\
\frac{-1}{c}\cdot (v-v) &= v\\
\frac{-1}{c}\cdot v+ \frac{1}{c}\cdot v &= v\\
v\cdot (1-1)&=v\\
v-v&=v\\
O&=v
\end{aligned}$$
\item \textbf{Exercise 3}
\label{sec:org8478d5d}

\(\forall g\in V, (g+f)(x) = g(x)+f(x) = f(x)+g(x) = (f+g)(x) \Rightarrow g+f = f+g\).\\
If \(O+u = u\), \((O+u)(x) = O(x)+u(x)= u(x)\). Therefore, \(O(x)=0\).
\item \textbf{Exercise 4}
\label{sec:orgc6f6a92}
$$\begin{aligned}
v+w&=O\\
v+w&=v+(-v)\\
w&=-v
\end{aligned}$$
\item \textbf{Exercise 5}
\label{sec:org25678a8}
$$\begin{aligned}
v+w&=v\\
v+(-v)+w&=v+(-v)\\
O+w&=O
\end{aligned}$$
Since \(\forall u, O+u=u\), we have \(w=O\).
\item \textbf{Exercise 6}
\label{sec:org2afb438}

Let \(W=\{B| B\cdot A_{1}=O\ \text{and}\ B\cdot A_2=O\}\). Specifically, it is clear that \(O\in W\) as \(O\cdot A = \sum\limits_{i=1}^{n} b_i a_i=\sum\limits_{i=1}^{n} 0\times a_i=0\).\\
Let \(v_1,v_2 \in W\) such that \(v_1\cdot A_1=0\), \(v_1\cdot A_2=0\), \(v_2\cdot A_1=0\), \(v_2\cdot A_2=0\). Thus,
$$\begin{aligned}
(v_1+v_2)\cdot A_1&=v_1\cdot A_1+v_2\cdot A_1\\
&=O+O\\
&=O\\
[c(v_1+v_2)]\cdot A_1&=(cv_1+cv_2)\cdot A_1\\
&=(cv_1)\cdot A_1+(cv_2)\cdot A_1\\
&=c(v_1\cdot A_1+v_2\cdot A_1)\\
&=cO\\
&=O
\end{aligned}$$.
It is easy to show for \(A_2\) then. Therefore, \((v_1+v_2)\in W\).
\item \textbf{Exercise 7}
\label{sec:orga658c7b}
Same to apply as Exercise 6.
\item \textbf{Exercise 8}
\label{sec:org4d26513}

Name the set as \(W\).
\begin{enumerate}
\item Proof
\label{sec:org8366447}

\(v_1+v_2=(x_1+x_2,y_1+y_2), x_1+x_2=y_1+y_2 \Rightarrow (v_1+v_2)\in W\) \\
\(cv=(cx,cy), cx=cy \Rightarrow cv\in W\) \\
\(O=(0,0)\in W\)
\item Proof
\label{sec:orge3fba92}
See Part (a).
\item Proof
\label{sec:orgdc53160}
Same technique as in Part (a).
\end{enumerate}
\item \textbf{Exercise 9}
\label{sec:orge5c43b9}
See Exercise 8.
\item \textbf{Exercise 10}
\label{sec:org039ce8d}

For \(U\cap W\), let \(v_1,v_2\in U\cap W\). Since \(v_1, v_2\in U\) and \(U\) is a subspace, \(v_1+v_2\in U\). In same way, we can see that \(v_{1}+v_2\in W\). Thus, \(v_1+v_2\in U\cap W\).\\
Since \(v_1\in U\), \(cv_1\in U\). Also, it shows \(cv_1\in W\) in the same way. Thus, \(cv_{1}\in U\cap W\).
Because \(U, W\) are subspaces, \(O\in U\) and \(O\in W\). Thus, \(O\in U\cap W\). Therefore, \(U\cap W\) is a subspace.\\
Refer to the \hyperref[org6f18be9]{note part} for proof for \(U+W\).
\item \textbf{Exercise 11}
\label{sec:org5567272}
Since \(L\) is a field, \textbf{VS1, VS3, VS4, VS8} are established under field axioms, and multiplication and addition are closed in \(L\). For \textbf{VS5, VS6, VS7}, they are all valid as \(K\subset L\). \(O\) is simply \(0\), and \(1\cdot u=u\) is  established in \(L\).
\item \textbf{Exercise 12}
\label{sec:org2c6797f}

For \(x,y\in K\), we have\\
\(x+y=a_1+b_1\sqrt{2}+a_2+b_2\sqrt{2}=(a_1+a_2)+(b_1+b_2)\sqrt{2}\). Since \(a_1,b_1,a_2,b_2\in \mathbb{Q}\), \((a_1+a_2),(b_1+b_2)\in\mathbb{Q}\). Thus, \(x+y\in K\).\\
\(xy=(a_1 a_2+ 2b_1 b_2)+(a_2 b_1 + a_1 b_2)\times \sqrt{2}\). Since \(a_1,b_1,a_2,b_2\in \mathbb{Q}\), \((a_1 a_2+ 2b_1 b_2),(a_2 b_1 + a_1 b_2)\in\mathbb{Q}\). Thus, \(x+y\in K\).\\
\(-x=-a+-b\sqrt{2}\). Since \(a,b\in\mathbb{Q}\), \(-a,-b\in\mathbb{Q}\). Thus, \(-x\in K\).\\
If \(a+b\sqrt{2}\not = 0\), \(a,b\not = 0\), and \(a-b\sqrt{2}\not = 0\). Thus, \(x^{-1}=\frac{1}{a+b\sqrt{2}}=\frac{a-b\sqrt{2}}{a^2-2b^{2}}=\frac{a}{a^2-2b^2}-\frac{b}{a^2-2b^2}\sqrt{2}\). It is easy to see that \textbf{new} \(a,b\in\mathbb{Q}\) as \(a,b\in\mathbb{Q}\). Thus, \(x^{-1}\in K\).
Specifically, if \(a=b=0\), \(0\in\mathbb{Q}\). If \(a=1,b=0\), \(1\in\mathbb{Q}\).\\
Thus, \(K\) is a field.
\item \textbf{Exercise 13}
\label{sec:org065b8b3}
Same technique as Exercise 12.
\item \textbf{Exercise 14}
\label{sec:org13ac6fc}
Same technique as Exercise 12.
\end{enumerate}
\subsection{\(\S\) 2}
\label{sec:orgf6f4008}
\subsubsection{Notes}
\label{sec:orgf6fc502}
\label{org9a76a6d}
Another quite helpful equivalent of definition of linear independence is that (stated following without loss of generality)
$$\forall a_i\in K \text{ and some } a_i\not = 0, \text{ we have }a_1 v_1\not =\sum\limits_{i=2}^n a_iv_i$$
Here is the \emph{proof} of equivalence between above statement and definition of linear independence.\\
$$\begin{aligned}
a_1 v_1&\not =\sum\limits_{i=2}^n a_iv_i\\
O &\not = \sum\limits_{i=1}^n a_iv_i
\end{aligned}$$
This means as long as \textbf{some} \(a_i\not =0\), \(O \not = \sum\limits_{i=1}^n a_iv_i\). In other words, only if all \(a_i=0\), \(O = \sum\limits_{i=1}^n a_iv_i\). This means any \(v_i\) fulfilling our statement are linear independent. Conversely, if \(v_i\) are linear independent, it is clear that as long as \textbf{not all} \(v_i=0\), \(a_1 v_1\not =\sum\limits_{i=2}^n a_iv_i\), which is equal to our statement.\\
A simple but useful variation of this is
$$\forall v_i\in K, v_1\not = \sum\limits_{i=2}^n x_iv_i$$
\emph{Proof}. We see that
$$\begin{aligned}
O&\not = -v_1+\sum\limits_{i=2}^n x_iv_i\\
O&\not = (-\lambda)v_1+\sum\limits_{i=2}^n \lambda x_iv_i && \lambda\not = 0\text{ (If } \lambda=0 \text{ inequality holds not)}
\end{aligned}$$
Since \(\lambda\) and \(v_i\) can be arbitrary and they cannot be \(0\) all at once, we see it falls into the case of original statement.\\
Also, another point that worth paying attention to is that generators could be \textbf{linear dependent}. This is true because you could put arbitrary vectors at the end of a basis of a vector space and just set coefficients for these extraneous vectors when it is producing new linear combinations.
\subsubsection{Exercises}
\label{sec:orgb3dd148}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:org7988335}
Using result from \hyperref[org2a66d8c]{\textbf{Exercise 4}}, easy to prove.
\item \textbf{Exercise 2}
\label{sec:orgaa4a238}
\begin{enumerate}
\item \((1,-1)\)
\label{sec:orgf05af31}
\item \((\frac{1}{2},\frac{3}{2})\)
\label{sec:orgb8c2e7e}
\item \((1,1)\)
\label{sec:orgf95442a}
\item \((3,2)\)
\label{sec:org5f52229}
\end{enumerate}
\item \textbf{Exercise 3}
\label{sec:orge4ad0ef}
\begin{enumerate}
\item \((\frac{1}{3},-\frac{1}{3},\frac{1}{3})\)
\label{sec:org555c33c}
\item \((1,0,1)\)
\label{sec:orgcf4dd19}
\item \((\frac{1}{3},-\frac{1}{3},-\frac{2}{3})\)
\label{sec:org1dbde04}
\end{enumerate}
\item \textbf{Exercise 4}
\label{sec:org958be2a}
\label{org2a66d8c}

Following set of equations is an equivalent of \(x(a,b)+y(c,d)=O\),
$$\begin{aligned}
ax+cy&=0 && (1)\\
bx+dy&=0 && (2)\\
\end{aligned}$$
$$\begin{aligned}
(1)\times d-(2)\times c\Rightarrow (ad-cb)x+cdy-cdy &= 0\\
(ad-cb)x&=0\\
\end{aligned}$$
For \(ad-cb\not =0\) part, clearly we shall see that \(x=0\) as \((ad-cb)x=0\). Plugging \(x\) back to \((1)\), we get \(y=0\). Thus, two vectors are linear independent.\\
For \(ad-cb=0\) part, we need to prove that \(x(a,b)+y(c,d)=O\) has solution other than \(x=y=0\).\\
First, suppose \(a,b,c,d\not = 0\). Since \(ad-cb=0\), \(x\in \mathbb{R}\). By applying technique, we could also show \(y\in \mathbb{R}\). Thus, \((a,b),\ (c,d)\) are linear independent.\\
If \(a,b,c,d\not = 0\) does \textbf{NOT} hold. Without lose of generality (for all the possibilities, \(a,d\) and \(c.b\) are interchangeable), consider following scenarios in a \(xy\) -plane,
\begin{enumerate}
\item \(a=0,c=0\)
\label{sec:org6d6322e}

If \(a=c=0\), \(x,y\in \mathbb{R}\) in \((1)\). Because the \((2)\) is a line in the plane, there must exist some \(x,y\not = 0\).
\item \(a=0,b=0,c=0\)
\label{sec:orgdfddebd}

Same argument as above, despite the line represented by \((2)\) is a little bit peculiar (it is \(y=0\)).
\item \(a=0,d=0,c=0\)
\label{sec:org0b23b7b}

Same argument as the first, despite the line represented by \((2)\) is a little bit peculiar (it is \(x=0\)).
\item \(a=0,d=0,b=0,c=0\)
\label{sec:org0287bf6}

Both \((1), (2)\) represent the whole plane, thus, \(x,y\in \mathbb{R}\).
\end{enumerate}
\item \textbf{Exercise 5,6}
\label{sec:orgc721d1c}

To correctly understand how could functions be elements(vectors) in vector space, we need to understand that function \(f:S\rightarrow K\) is essentially a set of pairs \((s,k),\forall s\in S\). Functions have scalar multiplication and addition defined.\\
\(f+g\) is defined as \(\{(s,f(s)+g(s))|s\in S\}\), and \(cf, c\in K\) is defined as \(\{(s,c\cdot f(s))|s\in S\}\).\\
It is easy to verify that \(V\) of every \(f:S \rightarrow K\) is a vector space over \(K\). Particularly, \(O\) for \(V\) is \(\{(s,0)|s\in S\}\). So like other vector spaces, linear dependence is \textbf{about}
$$f_{sum}=\sum\limits_{i=1}^n a_if_i=O$$
Since right-hand-side of the equation is \(\{(s,0)|s\in S\}\), we can say that \(\forall v\in V, f_sum (s)=0\). This is useful in solving problems in \textbf{Exercise 5} and \textbf{Exercise 6}.\\
For example, we need to show that \(f(s)=1\) and \(g(s)=t\) are linear independent. This means that we need to consider following equation,
$$af+bg=O$$
which is an equivalent of
$$\forall t,a+bt=0$$
Above conversion is quite helpful since we could put in arbitrary \(t\) and the equation should hold. Thus, we could put in particular values of \(t\) to \textbf{construct} set of equations to show that \(a=b=0\). For example, here we plug in \(t=0\), then \(a=0\), and if we plug back \(a=0\) into original equation with \(t=0\) again, \(b=0\).\\
This method could be used throughout \textbf{Exercise 5,6}.
\item \textbf{Exercise 7}
\label{sec:org7f2c590}
\((3,5)\)
\item \textbf{Exercise 8}
\label{sec:orgc93ee50}
\emph{\textbf{Calculus involved, not doing now.}}
\item \textbf{Exercise 9}
\label{sec:orgefaac1c}
$$\begin{aligned}
\sum\limits_{i=1}^{r} [a_i\cdot (A_i\cdot \sum\limits_{j=i+1}^{r}A_{j})]&=O && \text{All vectors are mutually perpendicular}\\
&=\sum\limits_{i=1}^{r} [(a_i\cdot A_i)\cdot \sum\limits_{j=i+1}^{r}A_{j}]\\
\end{aligned}$$
Since \(\forall A\in \{A_i\}, A\not = O\), it is only possible that every \(a\) is \(0\). Thus, \({A_i}\) are linearly independent.
\item \textbf{Exercise 10}
\label{sec:org64a728f}

Since \(v,w\) are linear dependent, for
$$nv+mw=O$$
at least one of \(n,m\not =0\).
Consider following scenarios, we can see that there would be \(a=0\) or \(a=-\frac{n}{m}\).
\begin{enumerate}
\item \(n=0,m\not =0 \Rightarrow w=O\)
\label{sec:org654ac78}
\item \(n\not =0,m =0 \Rightarrow v=O\).
\label{sec:org12a2e4f}
This contradicts with \(v\not =O\) in problem. Thus, this is impossible.
\item \(n\not = 0, m\not =0 \Rightarrow w=\frac{-n}{m}v\)
\label{sec:org0d496d4}
\end{enumerate}
\end{enumerate}
\subsection{\(\S\) 3}
\label{sec:orgaa02314}
\subsubsection{Notes}
\label{sec:orgf6dc4d7}
This subsection comprises a lot of concise proofs. But in conclusion, we need to know that
$$\begin{aligned}
\text{Basis}&\Leftrightarrow \text{Maximal linear independent vector set} && \text{proof at }\bold{Theorem 3.1}\\
\text{Basis}&\Leftrightarrow \text{Maximal linear indpendent vector set} \Rightarrow \text{Generators} && \text{proof at }\bold{Theorem 2.2}\\
\text{Generators} &\nRightarrow \text{Basis} && \text{Generators are not always linear independent.}
\end{aligned}$$
Thus, all possible bases of a vector space \(V\) are of one and only one possible number of elements, which is equal to the one of maximal independent vector set.
\subsection{\(\S\) 4}
\label{sec:org97ce977}
\subsubsection{Notes}
\label{sec:org93d9da6}
\emph{Proof} for $$\dim (U\times W)=\dim U+\dim W$$
Because \(\forall u\in (U\times W) ,(O_u+O_w)+u=u+(O_u+O_w)=u\). Thus, by definition, \(O=(O_u,O_w)\).\\
Let \(A=\{u_i\}\) be a basis of \(U\) and \(B=\{w_i\}\) be a basis of \(W\). Note the dimension of \(U,W\) as \(n,m\) respectively. Let $$C=\{(u_i,0)|u_i\in A\}\cup\{(0,w_i)|w_i\in B\}$$
Since there would be no intersection between two sets being union above, the number of elements in \(C\) is \(n+m\).
If we could show that \(C\) is a basis of \(U\times W\), then we could show the original statement.\\
First we need to show that all elements in \(C\) is linear independent. This means \(a_i\in K,c_i\in C\)
$$\sum\limits_{i=1}^{n+m}a_i c_i=O$$
if and only if all the \(a_i=0\).\\
Because multiplication by scalar and addition for \(U\times W\) is defined componentwise, we shall see that (if we keep the "order" of elements in \(C\) as \(A\) and \(B\) are merged)
$$\begin{aligned}
\sum\limits_{i=1}^{n}a_i u_i&=O_u\\
\sum\limits_{i=n+1}^{n+m}a_i w_i&=O_w
\end{aligned}$$
Since both \(A\) and \(B\) are basis of \(U\) and \(W\) respectively, all the \(a_i\) should be \(0\).\\
Now, we need to show that \(C\) generates \(U\times W\). Since \(A\) and \(B\) are basis of \(U\) and \(W\) respectively,
$$\forall (a,b)\in (U\times W), \exists f_i,g_i\in K:\sum\limits_{i=1}^n f_i u_i=a \text{ and } \sum\limits_{i=1}^m g_i w_i=b $$
Thus, by setting set of scalar for "order"-kept \(C\) as \(\{f_i\}\cup \{g_i\}\), it is easy to see that it generates \(U\times W\).\\
Therefore, we see that
$$\dim (U\times W)=\dim U+\dim W$$
and
$$\{(u_i,0)|u_i\in A\}\cup\{(0,w_i)|w_i\in B\}$$
is a basis for \(U\times W\).\\
\subsubsection{Exercises}
\label{sec:org6012de4}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:org8306e20}

For the first part, we need to show that \(\forall v\in V, \exists \text{ unique } u\in U, w\in W: v=u+w\). Since \((2,1)\) and \((0,1)\) are linear independent, they are a basis of \(V=\mathbb{R}^2\). This means
$$\forall v\in V, \exists \text{ unique } a,b\in K: v=a\cdot (2,1)+b\cdot (0,1)$$
Thus, just set \(u=a\cdot (2,1)\) and \(w=b\cdot (0,1)\), and we have proved it.\\
It is same for \((2,1)\) and \((1,1)\).
\item \textbf{Exercise 2}
\label{sec:org321b8ec}

Since \((1,0,0), (1,1,0), (0,1,1)\) are linear independent, we obtain that
$$\forall v\in V, \exists \text{ unique } a,b,c\in K: v=a\cdot (1,0,0)+b\cdot (1,1,0)+ c\cdot (0,1,1)$$
Set \(u=a\cdot (1,0,0)\) and \(w=b\cdot (1,1,0)+ c\cdot (0,1,1)\), it would be proved.
\item \textbf{Exercise 3}
\label{sec:orgde35387}
$$\begin{aligned}
cA&\not =B\\
O&\not = B-cA\\
O&\not =\lambda B-c\lambda A && \lambda\not = 0\text{ (If } \lambda=0 \text{ inequality holds not)}
\end{aligned}$$
Since \(\lambda ,c\) are arbitrary and \(\lambda\not = 0\), coefficients before \(A\) and \(B\) can be anything but not equal to \(0\) together.
According to argument provided \hyperref[org9a76a6d]{here}, \(A,B\) are linear independent. Also, according to \textbf{Theorem 3.4}, they are a basis of \(\mathbb{R}^2\).\\
Based on the similar argument in \textbf{Exercise 1}, second part could be proved.
\item \textbf{Exercise 4}
\label{sec:org8187728}
See notes
\end{enumerate}
\section{Chapter 2}
\label{sec:org02075a1}
\subsection{\(\S\) 1}
\label{sec:org31e9cf1}
\subsubsection{Exercises}
\label{sec:org36d5368}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:orgb41cb92}
Skip
\item \textbf{Exercise 2}
\label{sec:org6e8912a}
Skip
\item \textbf{Exercise 3}
\label{sec:org4ea8200}
Skip
\item \textbf{Exercise 4}
\label{sec:orgc75a75e}
Skip
\item \textbf{Exercise 5}
\label{sec:orgbadd659}
Let \(C=^t(A+B)=(c_{ij})\). Then, \(c_{ij}=(a_{ij}+b_{ij})'=a_{ji}+b_{ji}\). Thus, \(C=^tA+^tB\).
\item \textbf{Exercise 6}
\label{sec:org998902d}
Let \(B=^t(cA)\). Then, \(b_{ij}=ca_{ji}\). Since \(^tA=(a^{\prime}_{ij})=(a_{ji})=A\), \(B=c^tA\).
\item \textbf{Exercise 7}
\label{sec:org47034df}
No difference.
\item \textbf{Exercise 8}
\label{sec:org66f916e}
Skip
\item \textbf{Exercise 9}
\label{sec:org40f4899}
Skip
\item \textbf{Exercise 10}
\label{sec:org6210b67}
Let \(B=A+^tA=(b_{ij})=(a_{ij}+a_{ji})\). Since, \(b_{ij}=a_{ij}+a_{ji}=a_{ji}+b_{ij}=b_{ji}\), \(B\) is symmetric.
\item \textbf{Exercise 11}
\label{sec:org277ed05}
Skip
\item \textbf{Exercise 12}
\label{sec:org6c8745e}
Skip
\item \textbf{Exercise 13}
\label{sec:org9fb71b7}

\textbf{For followings, we mean ones in \emph{Exercises on Dimension} section}.\\
Followings are linear independent.
$$U_1=\begin{pmatrix}
   1 & 0 \\
   0 & 0
\end{pmatrix}$$
$$U_2=\begin{pmatrix}
   0 & 1 \\
   0 & 0
\end{pmatrix}$$
$$U_3=\begin{pmatrix}
   0 & 0 \\
   1 & 0
\end{pmatrix}$$
$$U_4=\begin{pmatrix}
   0 & 0 \\
   0 & 1
\end{pmatrix}$$
Apply \(a\cdot U_1+b\cdot U_2+c\cdot U_3+d\cdot U_4=O\) to verify it.
Because it generates the matrix vector space \(Mat_{2\times 2}K\) over K (For every \(v\in Mat_{2\times 2}K\), simply let \(a,b,c,d\) be \(v\)'s components) and \(\{U_i\}\) are linear independent, \(\{U_i\}\) is a basis of \(Mat_{2\times 2}K\).\\
Because the number of elements in a basis is the dimension of the vector space, we see that the dimension of it is 4.
\item \textbf{Exercise 14}
\label{sec:org65eed05}
Similar argument to \textbf{Exercise 13}. Dimension of it is \(mn\).
\item \textbf{Exercise 15}
\label{sec:org09f3105}
Dimension of it is \(n\). Simply build up a basis to see.
\item \textbf{Exercise 16}
\label{sec:org2060a09}
Similarly, dimension of it is \(\frac{(n+1)n}{2}\).
\item \textbf{Exercise 17}
\label{sec:org81efe86}

Basis is a set comprises
$$U_1=\begin{pmatrix}
   1 & 0 \\
   0 & 0
\end{pmatrix}$$
$$U_2=\begin{pmatrix}
   0 & 1 \\
   1 & 0
\end{pmatrix}$$
$$U_3=\begin{pmatrix}
   0 & 0 \\
   0 & 1
\end{pmatrix}$$
Then, it is easy to see that dimension is 3.
\item \textbf{Exercise 18}
\label{sec:org5a375f5}
Basis similar to the one in \textbf{Exercise 17} is linear independent and generates space. And, indeed, the number of elements in the basis is the same as one in \textbf{Exercise 16}. Thus, dimension of it is \(\frac{n(n+1)}{2}\).
\item \textbf{Exercise 19}
\label{sec:orgb1f1e7a}
Same as \textbf{Exercise 15}.
\item \textbf{Exercise 20}
\label{sec:org8a79ef8}

Let \(U\) be the subspace of \(V\). There would be a maximal number \(m\) of linear independent vectors (\textbf{Theorem 3.1} in chapter 1). Suppose the number \(m> \dim V\). Then it would contradicts \textbf{Theorem 3.1} in chapter 1 as any number of vectors more than \(\dim V\) would be linear dependent, which means the basis of \(U\) would be linear dependent (remember \(U\) is a subspace of \(V\)). Thus, \(m\leq \dim V\).\\
Dimension could be \(0,1,2\).
\item \textbf{Exercise 21}
\label{sec:org2f139f0}

According to the lemma we proved in \textbf{Exercise 20}, dimension of subspace of \(\mathbb{R}^{3}\) could be \(0,1,2,3\).
\end{enumerate}
\subsection{\(\S\) 2}
\label{sec:orgdae8277}
\subsubsection{Notes}
\label{sec:org88e251a}
\textbf{Lemma} Let \(A\) be a set of linear dependent vectors that generates \(V\). Then, for all \(v\in V\), there exists infinite linear combinations of \(A\) that form \(v\).\\
\emph{Proof} Say that number of vectors in \(A\) is \(n\). Since \(A\) generates \(V\), \(\forall v\in V, \exists \{a_i\}: v=\sum\limits_{i=1}^n a_i A_i\). Let \(L\) be a set of linear combinations that form \(v\) (here \(L\) is a set of sets). We have
$$\begin{aligned}
v&=\sum\limits_{i=1}^n a_iA_i+O\\
&=\sum\limits_{i=1}^n a_iA_i+\sum\limits_{i=1}^n b_iA_i\\
&=\sum\limits_{i=1}^n (a_i+b_i) A_i
\end{aligned}$$
Since \(A\) is linear dependent, there exists \(\{b_i\}\) where not every element is \(0\). Therefore, \(\{a_i+b_i\}\in L\) and \(\{a_i+b_i\}\not = \{a_i\}\) for some \(\{b_i\}\).\\
This means that \(\forall \ell \in L\), we can always form a new \(\ell^{\prime}\in L\). And since for all \(v\in V\) we always have one linear combination, we can do it infinitely, which means number of elements in \(L\) is infinite. Therefore, we have shown what was to be shown. \(\blacksquare\) \\
Here we discuss the number of solutions for general linear equations. (\(A\) is a \(m\times n\) matrix. \(X\) is a \(n\times 1\) column matrix. \(B\) is a \(m\times 1\) column matrix).
$$AX=B$$
If \(n>m\), according to \textbf{Theorem 3.1 in chapter 1}, they must be linear dependent, resulting in infinite number of solutions because of \textbf{Lemma} above.\\
If \(n=m\) and they are linear independent (it is then a basis because they are maximal independent vectors), there would only be one solution as \textbf{Theorem 2.1 in chapter 1} stated. If they are linear dependent and \(B\) is in the subspace generated by column vectors of \(A\), there would be infinite number of solutions (\textbf{Lemma}), else the equations are not solvable (there exists no linear combination to represent \(B\)).\\
If \(n<m\) and they are independent and \(B\) is in the subspace generated by column vectors of \(A\), there would be only one solution. If they are linear independent but \(B\) is not in subspace, then it is unsolvable. If they are linear dependent and \(B\) is in subspace, infinite solutions occur. If they are linear dependent but \(B\) is not in subspace, equations are not solvable.\\
In general,
\begin{enumerate}
\item If \(B\) is in the vector space generated by column vectors of \(A\) and they are linear independent, there exists one unique solution.
\label{sec:org23da5ec}
\item If \(B\) is in the vector space generated by column vectors of \(A\) and they are linear dependent, there exists infinite solutions.
\label{sec:orgca8f983}
\item If \(B\) is not in the vector space generated by column vectors of \(A\), there would be no solution.
\label{sec:org821b51d}
\end{enumerate}
\subsubsection{Exercises}
\label{sec:org9690a44}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:orgb055862}
See notes and refer to the definition of linear independence.
\item \textbf{Exercise 2}
\label{sec:org2f06e5e}

Let \(u\) be one set of solution and \(w\) be another.\\
We want to show that \(u+w\in X\).
$$\sum\limits_{i=1}^n (u_i+w_i)\cdot A^i=\sum\limits_{i=1}^n u_i\cdot A^i+\sum\limits_{i=1}^n w_i\cdot A^i=O+O=O$$
Thus, \(u+w\in X\).
Also, we need to show \(cu\in X\) where \(c\in K\).
$$c \sum\limits_{i=1}^n u_i \cdot A^i=cO=O$$
Other \textbf{VS} s are easy to follow as we define the addition of vectors in \(X\) componentwise, \(O\) as a vector whose components are all zero, \(1\) as a vector whose components are all one.
\item \textbf{Exercise 3}
\label{sec:org1c129dc}
We want to show following
$$\begin{aligned}
\sum\limits_{i=1}^n (a_i+b_i \text{i}) A^i&=O_{\mathbb{C}}\\
\sum\limits_{i=1}^n a_iA^{i}+\sum\limits_{i=1}^n b_i \text{i} \cdot A^i &= O_{\mathbb{C}}\\
O_{\mathbb{C}}+\sum\limits_{i=1}^n b_i \text{i} \cdot A^i&=O_{\mathbb{C}}\\
\sum\limits_{i=1}^n b_i \cdot A^i&=O_{\mathbb{C}}
\end{aligned}$$
This means that \(\{A^i\}\) should be linear independent over \(\mathbb{R}\) (\(\sum\limits_{i=1}^n b_i \cdot A^i=O_{\mathbb{C}}\) is equal to \(\sum\limits_{i=1}^n b_i \cdot A^i=O_{\mathbb{R}}\) as there is no imaginary part). Since it is known to us that \(\{A^i\}\) is linear independent over \(\mathbb{R}\), it has been proved as we do it reversely.
\item \textbf{Exercise 4}
\label{sec:org87318de}
We know that
$$\sum\limits_{i=1}^n (a_i+b_i \text{i}) A^i = O_{\mathbb{C}}$$
which means that \(\sum\limits_{i=1}^n a_i A^i = O_{\mathbb{C}}\) and/or \(\sum\limits_{i=1}^n b_i A^i = O_{\mathbb{C}}\).
For either cases, we have shown it is linear dependent over \(\mathbb{R}\) (\(a_i,b_i\in \mathbb{R}\)).
\end{enumerate}
\subsection{\(\S\) 3}
\label{sec:orgf58f030}
\subsubsection{Exercises}
\label{sec:orgda99633}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:org70713db}
\(AI=IA=A\)
\item \textbf{Exercise 2}
\label{sec:org20a484d}
\(AO=O\)
\item \textbf{Exercise 3}
\label{sec:org9c832dd}

For every \(A\) and \(B\), \((AB)C=A(BC)\).
\begin{enumerate}
\item Case 1
\label{sec:org35d6828}
$$\begin{pmatrix}
3&2\\
4&1
\end{pmatrix}$$
\item Case 2
\label{sec:org2fbf745}
$$\begin{pmatrix}
10\\
14
\end{pmatrix}$$
\item Case 3
\label{sec:org898c8c2}
$$\begin{pmatrix}
33&37\\
11&-18
\end{pmatrix}$$
\end{enumerate}
\item \textbf{Exercise 4}
\label{sec:org02a35f4}
This one could be proved as it is proved \hyperref[org5da29d8]{here}.
\item \textbf{Exercise 5}
\label{sec:orge2f86c9}
$$AB=\begin{pmatrix}
4&2\\
5&-1
\end{pmatrix}$$
$$BA=\begin{pmatrix}
2&4\\
4&1
\end{pmatrix}$$
\item \textbf{Exercise 6}
\label{sec:orge193ad3}
$$CA=AC=\begin{pmatrix}
7&14\\
21&-7
\end{pmatrix}$$
$$CB=BC=\begin{pmatrix}
14&0\\
7&7
\end{pmatrix}$$
General rule is that for symmetric one, we may have \(AB=BA\)? (I am not sure here).
\item \textbf{Exercise 7}
\label{sec:org3b4d5ad}
$$XA=\begin{pmatrix}
3&1&5
\end{pmatrix}$$
\item \textbf{Exercise 8}
\label{sec:orgaf50b2c}
$$\begin{aligned}
X_1 A=A_2\\
X_2 A=A_3
\end{aligned}$$
Let \(X_i\) be a unit vector with only \(i\) -th component equal to \(1\). \(X_i A=A_i\)
\item \textbf{Exercise 9}
\label{sec:org2c200f4}

Skip the steps involving verifications.
\(^t (AB)=^tB^tA\) has already been proved in \(\S2\). Thus, \(^t [(AB)C]=^t C\cdot ^t (AB)=^t C\cdot ^t B\cdot ^tA\).
\item \textbf{Exercise 10}
\label{sec:org063295c}

Firstly, we know \(A\) is of \(1\times n\), \(M\) is of \(n\times n\) and \(B\) is of \(1\times n\). This means that \(\dim (\langle A,B\rangle)=1\). Also, it implies that \(^t (\langle A,B\rangle)=\langle A,B\rangle\). Thus, we have
$$\begin{aligned}
\langle A,B\rangle&=^t (\langle A,B\rangle)\\
&=^t(AM^tB)\\
&=^t(^tB)\cdot ^tM\cdot ^tA && \text{Exercise 9}\\
&=BM^tA\\
&=\langle B,A\rangle
\end{aligned}$$
which is \textbf{SP 1}.
Also, let
$$N=^t(B+C)$$
Then, \(n_{ij}=n^{\prime}_{ji}=b_{ji}+c_{ji}\). This implies also \(N=^tA+^tB\). Therefore,
$$\langle A,B+C\rangle=AM^t(B+C)=AM(^tB+^tC)=\langle A,B\rangle+\langle A,C\rangle$$
which is \textbf{SP 2}. Finally
$$\langle cA,B\rangle=cAM^tB=c\langle A,B\rangle$$
which is \textbf{SP 3}.
\item \textbf{Exercise 11}
\label{sec:orgb73eba9}

For part \textbf{(a)}, see \textbf{Exercise 35}.\\
Part \textbf{(b)}
$$A^2=\begin{pmatrix}
1&2&3\\
0&1&2\\
0&0&1
\end{pmatrix}$$
$$A^3=\begin{pmatrix}
1&3&6\\
0&1&3\\
0&0&1
\end{pmatrix}$$
$$A^4=\begin{pmatrix}
1&4&10\\
0&1&4\\
0&0&1
\end{pmatrix}$$
\item \textbf{Exercise 12}
\label{sec:orgeabc5e2}
$$(AX)_{a}=\begin{pmatrix}
4\\
7\\
5
\end{pmatrix}
(AX)_{b}=\begin{pmatrix}
3\\
1
\end{pmatrix}$$
$$(AX)_c=\begin{pmatrix}
x_2\\
0
\end{pmatrix}
(AX)_d=\begin{pmatrix}
0\\
x_1
\end{pmatrix}$$
\item \textbf{Exercise 13}
\label{sec:orgc125bdf}
$$(AX)_a=\begin{pmatrix}
2\\
4
\end{pmatrix}$$
$$(AX)_b=\begin{pmatrix}
4\\
6
\end{pmatrix}$$
$$(AX)_c=\begin{pmatrix}
3\\
5
\end{pmatrix}$$
\item \textbf{Exercise 14}
\label{sec:orgb7d72c0}
$$(AX)_a=\begin{pmatrix}
3\\
1\\
2
\end{pmatrix}$$
$$(AX)_b=\begin{pmatrix}
12\\
3\\
9
\end{pmatrix}$$
$$(AX)_c=\begin{pmatrix}
5\\
4\\
8
\end{pmatrix}$$
\item \textbf{Exercise 15}
\label{sec:org54a680d}
\(AX=A^2\) (second column of \(A\)).
\item \textbf{Exercise 16}
\label{sec:orgfbb3167}
\(AX=A^i\)
\item \textbf{Exercise 17}
\label{sec:org819f718}

Let \(U_i\) be a unit column vector which only has \(1\) on its \(i\) -th component. The proposed form of \(C^k\) could be written in the following way.
$$\begin{aligned}
C^k&=\sum\limits_{i=1}^n b_{ik}A^i\\
&=\sum\limits_{i=1}^n b_{ik}[\sum\limits_{j=1}^m (a_{ji}\cdot U_j)]\\
&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^m a_{ji}b_{ik}\cdot U_j]\\
\end{aligned}$$
$$\begin{aligned}
C^k&=\sum\limits_{j=1}^m A_j\cdot B^k\cdot U_j\\
&=\sum\limits_{j=1}^m [\sum\limits_{i=1}^n a_{ji}b_{ik}\cdot U_j]
\end{aligned}$$
Two forms are essentially the same if you expand them and compare. Thus, we have proved that the proposed formula is an equivalence of the original definition.
\item \textbf{Exercise 18}
\label{sec:org77a58a0}
\begin{enumerate}
\item \(A^{-1}=(I+A)\Rightarrow A\cdot A^{-1}=I^2-A^2=I\)
\label{sec:org4230635}
\item \(A^{-1}=(I^2+IA+A^2)\Rightarrow A\cdot A^{-1}=I^3-A^3=I\)
\label{sec:orgca0c4f0}
\item For real number \(I\) and \(A\), we see that \(I^n-A^n\) can be factored into \(I-A\) and another polynomial, because according to remainder theorem, plugging in \(I=A\) results in \(I^n-A^n=0\). Thus, we could follow the same pattern to construct always a \(A^{-1}\).
\label{sec:orge61a4e4}
\item Set \(A^{-1}=(-A-2I)\)
\label{sec:orgd95587d}
\item Set \(A^{-1}=(-A^2-A)\)
\label{sec:orgf115128}
\end{enumerate}
\item \textbf{Exercise 19}
\label{sec:org85a5935}
$$AB=\begin{pmatrix}
1 & ab\\
0 & 1
\end{pmatrix}$$
$$A^2=\begin{pmatrix}
1 & 2a\\
0 & 1
\end{pmatrix}$$
Inductive step:
$$\begin{aligned}
A^{n+1}&=A^n\cdot A\\
&=\begin{pmatrix}
1 & na\\
0 & 1
\end{pmatrix}\cdot A\\
&=\begin{pmatrix}
1 & (n+1)a\\
0 & 1
\end{pmatrix}
\end{aligned}$$
Thus, we have proved it.
\item \textbf{Exercise 20}
\label{sec:orgc94c06d}
$$A^{-1}=\begin{pmatrix}
1 & -a \\
0 & 1
\end{pmatrix}$$
\item \textbf{Exercise 21}
\label{sec:org69bf253}
We now show that \(B^{-1}A^{-1}\) would be an inverse of \(AB\).
$$(AB)(B^{-1}A^{-1})=A(B\cdot B^{-1})A^{-1}=A\cdot A^{-1}=I$$
And for the reverse, it is easy to verify either.
\item \textbf{Exercise 22}
\label{sec:org71d0ca0}
See the solution manual
\item \textbf{Exercise 23}
\label{sec:org78d3ec5}
$$\begin{aligned}
A^2&=A\cdot A\\
&=\begin{pmatrix}
\cos 2\theta & -\sin 2\theta\\
\sin 2\theta & \cos 2\theta
\end{pmatrix}
\end{aligned}$$
Inductive step:
$$\begin{aligned}
A^{n+1}&=\begin{pmatrix}
\cos n\theta & -\sin n\theta\\
\sin n\theta & \cos n\theta
\end{pmatrix}\cdot A\\
&=\begin{pmatrix}
\cos n\theta \cos \theta - \sin n\theta \sin \theta & -(\sin n\theta \cos \theta + \sin \theta \cos n\theta) \\
\sin n\theta \cos \theta + \sin \theta \cos n\theta & - \sin n\theta + \cos n\theta \cos \theta
\end{pmatrix}
&=\begin{pmatrix}
\cos (n+1)\theta & -\sin (n+1)\theta\\
\sin (n+1)\theta & \cos (n+1)\theta
\end{pmatrix}
\end{aligned}$$
Thus, we have determined \(A^n\)
\item \textbf{Exercise 24}
\label{sec:orgddb69e9}
$$A=\begin{pmatrix}
0 & 1\\
-1 & 0
\end{pmatrix}$$
\item \textbf{Exercise 25}
\label{sec:orgaeb6cfe}
\begin{enumerate}
\item \(\text{tr}(A)=2\)
\label{sec:orgd55c6a2}
\item \(\text{tr}(A)=4\)
\label{sec:org480ebb4}
\item \(\text{tr}(A)=8\)
\label{sec:org478af36}
\end{enumerate}
\item \textbf{Exercise 26}
\label{sec:org57e3858}
See \textbf{Exercise 27}.
\item \textbf{Exercise 27}
\label{sec:orgcb1b465}
$$\begin{aligned}
\text{tr}(AB)&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^n a_{ij} b_{ji}]\\
&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^n b_{ji} a_{ij}]\\
&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^n b_{ij} a_{ji}] && \text{They are the same if you expand}\\
&=\text{tr}(BA)
\end{aligned}$$
\item \textbf{Exercise 28}
\label{sec:org786a3cf}
As diagonal line keeps same after transpose, trace of the matrix would not change as well.
\item \textbf{Exercise 29}
\label{sec:orgd44da63}
\(A^n=((a_{ij})^n)\)
\item \textbf{Exercise 30}
\label{sec:org02f3a08}
$$A^2=\begin{pmatrix}
a_1^2 & 0 & \cdots & 0\\
0 & a_2^2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n^2
\end{pmatrix}$$
Inductive step
$$A^{k+1}=\begin{pmatrix}
a_1^k & 0 & \cdots & 0\\
0 & a_2^k & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n^k
\end{pmatrix}\cdot A=\begin{pmatrix}
a_1^{k+1} & 0 & \cdots & 0\\
0 & a_2^{k+1} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n^{k+1}
\end{pmatrix}$$
\(\blacksquare\)
\item \textbf{Exercise 31}
\label{sec:orge95c258}
See \textbf{Exercise 35}
\item \textbf{Exercise 32}
\label{sec:orgc3208d1}
We want to show
$$\begin{aligned}
^t(A^{-1})&=(^t A)^{-1}\\
^t(A^{-1})\cdot ^t (A)&=(^t A)^{-1} \cdot (^tA)\\
^t(A^{-1})\cdot ^t (A)&=I_n\\
\end{aligned}$$
Let \(C=^t(A^{-1})\cdot ^t (A)\). We then know
$$\begin{aligned}
c_{ij}&=\sum\limits_{k=1}^n a^{\prime -1}_{ik} a^{\prime}_{kj}\\
&=\sum\limits_{k=1}^n a_{jk} a^{-1}_{ki}\\
&=A_j\cdot A^{-1\ i}
\end{aligned}$$
Thus,
$$\begin{aligned}
C&=^t(A\cdot A^{-1})\\
&=^t(I_n)=I_n
\end{aligned}$$
If we do it in the reverse way, then we can prove it.
\item \textbf{Exercise 33}
\label{sec:org1e2a725}
Let \(B=^t (\bar{A})\), then \(b_{ij}=\bar{a}_{ji}\). Let \(C=\overline{^t A}\), then \(c_{ij}=\bar{a^{\prime}}_{ij}=\bar{a}_{ji}\). Thus, \(B=C\).
\item \textbf{Exercise 34}
\label{sec:orgab7ab99}
Its inverse is
$$\begin{pmatrix}
\frac{1}{a_1} & 0 & \cdots & 0\\
0 & \frac{1}{a_2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \frac{1}{a_n}
\end{pmatrix}$$
\item \textbf{Exercise 35}
\label{sec:orgf99b135}
See solution manual. Here I would not like to introduce complex formal reasoning to simulate computation result.
\item \textbf{Exercise 36}
\label{sec:orgfa61aa7}

By result of \textbf{Exercise 35} we see that \(N^{n+1}=O\) as \(N=A-I_n\) is of the form being described in \textbf{Exercise 35}.\\
For inverse part, see \textbf{Exercise 18}.
\item \textbf{Exercise 37}
\label{sec:orgfd1b651}
$$(I-N)(I+N+\cdot +N^r)=I^{r+1}-N^{r+1}=I^{r+1}=I$$
\item \textbf{Exercise 38}
\label{sec:org96be8aa}
See solution manual for detail computation.
\item \textbf{Exercise 39}
\label{sec:orga8d40c6}
Since we know \(AB=BA\) or \(A,B\) fulfills \textbf{SP 1}, we may say
$$(AB)^r=A^rB^r=O$$
For \((A+B)\), we discuss \((A+B)^{2r}\) where \(r\) is the larger \(r\) for \(A\) and \(B\).
$$(A+B)^{2r}=\sum _{k=0}^{2r}{\binom {2r}{k}}A^{2r-k}B^{k}$$
If \(1\geq k\leq r\), then \(2r-k\geq r\) and \(A^{2r-k}=O\). If \(r<k \leq 2r\), then \(B^k=O\). Thus, essentially, \((A+B)^{2r}=O\).
\end{enumerate}
\section{Chapter 3}
\label{sec:org667de34}
\subsection{\(\S\) 1}
\label{sec:org46fefe1}
\subsubsection{Notes}
\label{sec:org6d22267}
If we want to say that \(S\) is the image of \(A\) under \(F\), we are essentially trying to say followings:
$$\begin{aligned}
&\forall z\in S, \exists x:F(x)=z. \Rightarrow S\subset F(A)\\
&\forall a\in A, F(a)\in S. \Rightarrow F(A)\subset S
\end{aligned}$$
Above are exactly what \textbf{Example 6} on Pg. 45 are saying.\\
Also, we shall work on the equality of two linear mappings. Two linear mappings \(F:S_1\rightarrow T_1 ,G:S_2\rightarrow T_2\) are said to be equal if and only if followings are fulfilled:
$$\begin{aligned}
S_1&=S_2\\
T_1&=T_2\\
\forall z\in S_1, F(z)&=G(z)
\end{aligned}$$
Proofs left to readers on Pg. 49.\\
 \emph{If \(u_1,u_2\) are elements of \(V\), then \(T_{u_1+u_2}=T_{u_1}\circ T_{u_2}\).}\\
$$\begin{aligned}
\forall v\in V, T_{u_1+u_2}&=(u_1+u_2)+v\\
&=u_1+(u_2+v)\\
&=T_{u_1}(u_2+v)\\
&=T_{u_1}(T_{u_2}(v))\\
&=T_{u_1}\circ T_{u_2}(v)
\end{aligned}$$
Which means that \(T_{u_1+u_2}=T_{u_1}\circ T_{u_2}\) according to our definition of linear mapping equality.\\
\emph{If \(u\) is an element of \(V\), then \(T_u:V\rightarrow V\) has an inverse mapping which is nothing but the translation \(T_{-u}\).}\\
First, it is easy to verify that \(T_{-u}\) is an inverse of \(T_u\). Then, we say that there is an inverse \(T_u^{-1}\). According to the definition of inverse of a linear mapping, we have, for every \(v\in V\) that
$$\begin{aligned}
T_u^{-1}(T_u(v))&=I_V(v)=v\\
T_u^{-1}(v+u)&=v\\
T_u^{-1}(x)&=x-u && \text{Let } x=v+u\\
\end{aligned}$$
which attests \(T_{u}^{-1}=T_{-u}\).\\
Here comes words on bijectivity, inverse and function composition:\\
\begin{enumerate}
\item For two mappings \(F:S_1\rightarrow T_1\) and \(F:S_2\rightarrow T_2\), \(F\circ G\) is only defined if \(T_1=S_2\).\\
\item A more clear proof for \emph{If \(F:S\rightarrow V\) has an inverse \(G:V\rightarrow S\), then \(F\) is bijective.} \emph{Proof.} If \(F(x)=F(y)\) given \(x,y\in S\), then \(G(F(x))=G(F(y))\). Also, since \(F,G\) are inverse of each other, we have
\end{enumerate}
$$\begin{aligned}
&\forall s\in S, (G\circ F)(s)=G(F(s))=I_s(s)=s\\
&\forall v\in V, (F\circ G)(v)=F(G(v))=I_v(v)=v
\end{aligned}$$
which means \(x=G(F(x))=G(F(y))=y\). Also, we contend that \(\forall v\in V, \exists x: F(x)=v\). Since we know \(\forall v\in V, (F\circ G)(v)=F(G(v))=I_v(v)=v\), we can simply let \(x=G(v)\) so that \(F(x)=v\). This proves the theorem.
\subsubsection{Exercises}
\label{sec:orge006821}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:orgb9da707}
\emph{\textbf{Calculus involved, not doing now.}}
\item \textbf{Exercise 2}
\label{sec:org454a002}
Proved in notes.
\item \textbf{Exercise 3}
\label{sec:orgb6ac873}
\begin{enumerate}
\item \(L(X)=11\)
\label{sec:org5349ac1}
\item \(L(X)=13\)
\label{sec:org39b2239}
\item \(L(X)=6\)
\label{sec:orge90ec5e}
\end{enumerate}
\item \textbf{Exercise 4}
\label{sec:org4dbe457}

\(F(1)=(e,1)\), \(F(0)=(1,0)\), \(F(-1)=(e^{-1},-1)\)
\item \textbf{Exercise 5}
\label{sec:org2ed54e6}

\((F+G)(1)=(e+1,3)\), \((F+G)(2)=(e^2+2,6)\), \((F+G)(0)=(1,0)\)
\item \textbf{Exercise 6}
\label{sec:org182c220}

\((2F)(0)=(2,0)\), \((\pi F)(1)=(\pi e,\pi)\)
\item \textbf{Exercise 7}
\label{sec:org5704a93}
For (a), it is 1. For (b), it is 11.
\item \textbf{Exercise 8}
\label{sec:org1ccf090}

The image is a ellipse of the equation
$$\frac{u^2}{4}+\frac{w^2}{9}=1$$
Proof is omitted.
\item \textbf{Exercise 9}
\label{sec:org27eb0ad}

The image is a straight line
$$y=\frac{1}{2}x$$
\emph{Proof.} \(A=\{(2,y)|y\in \mathbb{R}\}\), \(S=\{(2x,x)|x\in \mathbb{R}\}\). We contend that \(\forall a\in A, F(a)\in S\).
$$\forall y\in \mathbb{R}, F(2,y)=(2y,y)\in S$$
Conversely, \(\forall s=(x,\frac{1}{2}x)\in S\), let \(a=(2,\frac{1}{2}x)\in A\), so that \(F(a)=s\), which means \(S\subset F(A)\).
\item \textbf{Exercise 10}
\label{sec:org047a5a5}
It is a circle of center \((0,0)\) and radius \(e^c\). Proof is omitted.
\item \textbf{Exercise 11}
\label{sec:orgf9b654b}

It is a cylinder of radius 1 and center \((0,0)\). Proof is omitted.
\item \textbf{Exercise 12}
\label{sec:orgf1225e8}
\(x^2+y^2=1\). Proof is omitted.
\end{enumerate}
\subsection{\(\S\) 2}
\label{sec:org005ac5a}
\subsubsection{Notes}
\label{sec:org86d6e3f}
\label{org21f89ed}
Here we have an important \textbf{Lemma}\\
\emph{Let \(F:V\rightarrow W\) is a linear mapping. If for some \(v_i\in V\), we have \(F(v_i)\) are linear independent, then \(v_i\) are linear independent.}\\
\emph{Proof.} If \(\sum\limits_{i=1}^n t_i v_i=O\), then we have
$$\begin{aligned}
\sum\limits_{i=1}^n t_i v_i&=O\\
F(\sum\limits_{i=1}^n t_i v_i)&=F(O)=O && \text{(This is ensured as "output" of a mapping is unique for same "input")}\\
\sum\limits_{i=1}^n t_i F(v_i)=O
\end{aligned}$$
which means if \(\sum\limits_{i=1}^n t_i v_i=O\), we must have \(\sum\limits_{i=1}^n t_i F(v_i)=O\). Since \(F(v_i)\) are linear independent, we obtain that \(t_i\) is always equal to \(0\), which is another word for \(v_i\) are linear independent.\\
It is noteworthy that reversal of this \textbf{Lemma} is \textbf{NOT} always true as \(F(v)=O\) doesn't ensure that \(v=O\). In fact, in later subsections, we shall see that \(F\) is injective if and only if \(\text{Ker } F={O}\).
\subsubsection{Exercises}
\label{sec:org839a36e}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:orge97b215}
Only (a), (b), (d), (e), (f), (h) are linear mappings. For (h), it involves \textbf{\emph{Calculus}}.
\item \textbf{Exercise 2}
\label{sec:orgbc9ef76}
\(T(O)=T[v+(-v)]=T(v)+T(-v)=T(v)-T(v)=O\)
\item \textbf{Exercise 3}
\label{sec:orgaebbf20}
\(T(u+v)=T(u)+T(v)=w+O=w\)
\item \textbf{Exercise 4}
\label{sec:org4939134}

Let the set of elements \(v\in V\) satisfying \(T(v)=w\) be \(S\). We contend that \(\forall v\in S, \exists u\in U:v=u+v_0\).\\
\emph{Proof.} let \(u=v-v_0\). \(F(u)=F(v-v_0)=F(v)-F(v_0)=O\). Thus, \(v\in U\). This means \(S\subset (v_0+U)\).\\
Conversely, we contend that \(\forall u\in U\), we have \((v_0+u)\in S\).\\
\emph{Proof.} \(T(v_0+u)=T(v_0)+T(u)=w+O=w\). This means \((v_0+U)\subset S\).\\
Thus, \(S=(v_0+U)\).
\item \textbf{Exercise 5}
\label{sec:orgaa7081d}
As \textbf{Exercise 2} said, \(T(O)=T(v-v)=T(v)+T(-v)=O \Rightarrow T(-v)=-T(v)\).
\item \textbf{Exercise 6}
\label{sec:org514b5cb}

Firstly, \(F(v_1+v_2)=(f(v_1)+f(v_2),g(v_1)+g(v_2))=(f(v_1),g(v_1))+(f(v_2),g(v_2))=F(v_1)+F(v_2)\) \\
Secondly, \(F(cv)=(cf(v),cg(v))=c(f(v),g(v))=cF(v)\)
\item \textbf{Exercise 7}
\label{sec:orgd9ef7b5}
\begin{enumerate}
\item Prove \((u_1+u_2)\in U\). We have \(F(u_1+u_2)=F(u_1)+F(u_2)=O+O=O\).
\item Prove \(cu\in U\). We have \(F(cu)=cF(u)=O\).
\end{enumerate}
\item \textbf{Exercise 8}
\label{sec:orgac9de2f}
Mapping 8 is linear, others are not.
\item \textbf{Exercise 9}
\label{sec:org022c05b}

By definition later introduced in \(\S 5\), it is line segment between \(F(v)\) and \(F(v+w)\).\\
If \(F(w)\not = O\), then it is a line segment. If \(F(w)=O\), then it is a point.
\item \textbf{Exercise 10}
\label{sec:org3f37ddd}
By definition, it is a parallelogram.
\item \textbf{Exercise 11}
\label{sec:orgf3a358e}
Note that \(E_1,E_2\) are standard generators.
Since \(S\) is a set of points that can be written in the form \(t_1E_1+t_2E_2\) where \(0\leq t_1\leq 1\) and \(0\leq t_2\leq 1\). Thus, \(F(t_1E_1+t_2E_2)=t_1F(E_1)+t_2F(E_2)\) where \(0\leq t_1\leq 1\) and \(0\leq t_2\leq 1\). Hence, prove the statement.
\item \textbf{Exercise 12}
\label{sec:orge27a82c}
We know \(3E_1\) and \(E_2\) are also linear independent. So are \(3F(E_1)\) and \(F(E_2)\). Thus, adopting similar reasoning in \textbf{Exercise 11}, we prove statement.
\item \textbf{Exercise 13}
\label{sec:orgcc80cb2}
It is a parallelogram generated by \(5A\) and \(2B\).
\item \textbf{Exercise 14}
\label{sec:org9168fc7}
\(T_u(v_1+v_2)=v_1+v_2+u=T_u(v_1)+T_u(v_2)=v_1+v_2+2u\). Thus, we have \(2u=u\) and \(u=O\).
\item \textbf{Exercise 15}
\label{sec:org822a8a8}
It is shown in \hyperref[org21f89ed]{Lemma}.
\item \textbf{Exercise 16}
\label{sec:orge28e553}

If \(v\in W\), simply let \(c=0\) and \(w=v\).\\
If \(v\not \in W\), let \(c=\frac{F(v)}{F(v_0)}\) and \(w=v-cv_0\). We then contend that \(w\in W\). Since \(F(w)=F(v-cv_0)=F(v)-cF(v_0)=0\), we conclude \(w\in W\). Thus, concludes.
\item \textbf{Exercise 17}
\label{sec:org9b09cb8}

We see that \(F(w_1+w_2)=F(w_1)+F(w_2)=0+0=0\). Also, \(F(cw)=cF(w)=0\). And since \(F\) is linear and as we proved before, \(F(O)=O\) and \(O\in W\). Thus, \(W\) is a subspace of \(V\).\\
We know by \textbf{Exercise 16} that \(\{v_0,v_1,\cdots , v_n\}\) generates \(V\). We contend then that they are linear independent. It is by definition that \(\{v_1,\cdots , v_n\}\) are linear independent. Since \(v_0\not \in W\), we see that \(v_0\) cannot be expressed by linear combination of \(\{v_0,v_1,\cdots , v_n\}\), \hyperref[org9a76a6d]{thereby} \(v_0\) is linear independent from others. Thus, they are linear independent. Then, by definition, this set is a basis of \(V\).
\item \textbf{Exercise 18}
\label{sec:org8a13df1}
\begin{enumerate}
\item \((-1,-1)\)
\item \((-\frac{2}{3},1)\)
\item \((-2,-1)\)
\end{enumerate}
\item \textbf{Exercise 19}
\label{sec:orgff4de0f}
\begin{enumerate}
\item \((4,5)\)
\item \((\frac{11}{3},-3)\)
\item \((4,2)\)
\end{enumerate}
\end{enumerate}
\subsection{\(\S\) 3}
\label{sec:orga8e07f7}
\subsubsection{Notes}
\label{sec:orgfd538c0}
Another part for \textbf{Theorem 3.3}. If \(\text{Im } L=W\), then \(\dim \text{Im } L=\dim W\) and \(\dim \text{Ker }L=0\). Thus, \(\text{Ker } L=\{O\}\).
\subsubsection{Exercises}
\label{sec:orgaeffbd5}
\begin{enumerate}
\item \textbf{Exercise 1}
\label{sec:org17ddacf}

We know \(\dim \mathbb{R}^{n}=n\). According to rank-nullity law, we see that \(\dim \mathbb{R}^2=\dim \text{Ker }F+\dim \text{Im }F\). Thus, \(2=\dim \text{Ker }F+n\) and \(2-n=\dim \text{Ker }F\) Since \(n,\dim \text{Ker }F\geq 0\), we see that \(0\leq n\leq 2\).
\begin{enumerate}
\item \(n=2\), then \(\dim \text{Ker }F=0\) and \(\text{Ker }F=\{O\}\). This means that \(t_1F(A)+t_2F(B)=O \Rightarrow F(t_1A+t_2B)=O \Rightarrow t_1A+t_2B=O \Rightarrow t_1=t_2=0\).
\item \(n=1\), then \(\dim \text{Im }F=1\).
\item \(n=0\), then \(\dim \text{Im }F=0\) and \(\text{Im }F=\{O\}\).
\end{enumerate}
\item \textbf{Exercise 2}
\label{sec:orgf34f63c}

We know then that \(\dim \text{Ker }F\not =0\). Since \(2-\dim \text{Ker } F=\dim \text{Im }F\), we see that \(\dim \text{Im }F=0\) or \(1\). This concludes our prove.
\item \textbf{Exercise 3}
\label{sec:org50251f0}
Consider \(L:\mathbb{R}^4\rightarrow \mathbb{R}^2\) such that \(L(x_1,x_2,x_3,x_4)=L(x_1+2x_2,x_3-15x_4)\). According to rank-nullity theorem and since \(\text{Ker }L=W\), we see that \(\dim \mathbb{R}^4=\dim W+\dim \mathbb{R}^{2}\). Thus \(\dim W=2\).
\item \textbf{Exercise 4}
\label{sec:orgf12cb63}
We contend that there exists such a \(u\). \(\forall X\), \(L(X-v_0)=L(X)-L(v_0)=O\). This means if we let \(u=X-v_0\), then \(u\in \text{Ker }L\).
\item \textbf{Exercise 5-9}
\label{sec:org7ace46b}
\emph{\textbf{Calculus Involved, not done now.}}
\item \textbf{Exercise 10}
\label{sec:org5b766b5}

\begin{enumerate}
\item Let such a subspace be \(W\). Consider \(L:\mathbb{R}^n\rightarrow \mathbb{R}\) such that \(L(X)=\sum\limits_{i=1}^n x_i\). According to rank-nullity theorem and since \(\text{Ker }L=W\), we see that \(\dim \mathbb{R}^n=\dim W+\dim \mathbb{R}\). Thus \(\dim W=n-1\).
\item Let such a subspace be \(W\). Consider \(tr:\text{Mat}_{n\times n}(\mathbb{R})\rightarrow \mathbb{R}\) such that \(tr(A)=\sum\limits_{i=1}^n a_ii\). According to rank-nullity theorem and since \(\text{Ker }tr=W\), we see that \(\dim \text{Mat}_{n\times n}(\mathbb{R})=\dim W+\dim \mathbb{R}\). Thus \(\dim W=n^2-1\).
\end{enumerate}
\item \textbf{Exercise 11}
\label{sec:org9987eef}
\begin{enumerate}
\item We have \(tr(A+B)=\sum\limits_{i=1}^n(a_{ii}+b_{ii})=\sum\limits_{i=1}^n a_{ii}+\sum\limits_{i=1}^n b_{ii}=tr(A)+tr(B)\) and \(tr(cA)=\sum\limits_{i=1}^n(ca_{ii})=c\sum\limits_{i=1}^n a_{ii}=c\cdot tr(A)\). This concludes linearity for \(tr\).
\item $$\begin{aligned}
   tr(AB)&=\sum\limits_{i=1}^n A_iB^i\\
   &=\sum\limits_{i,j=1}^n a_{ij}b_{ji}\\
   &=\sum\limits_{i,j=1}^n b_{ji}a_{ij}\\
   &=\sum\limits_{i=1}^{n} B_iA^i\\
   &=tr(BA)\\
   \end{aligned}$$
\item Since we know \(tr(AB)=tr(BA)\), we have \(tr[(B^{-1}A)B]=tr[B(B^{-1}A)]=tr[(BB^{-1})A]=tr(I_nA)=tr(A)\)
\item Firstly, \(\langle A,B \rangle = tr(AB)=tr(BA)=\langle B,A \rangle\). Secondly, \(\langle A,B+C \rangle=tr[A(B+C)]=tr[AB+AC]=tr(AB)+tr(AC)=\langle A,B\rangle+\langle A,C\rangle\). Thirdly, \(c\langle A,B \rangle=c\sum\limits_{i=1}^n A_iB^i=\sum\limits_{i=1}^n (cA_i)B^i=\langle cA,B \rangle\).
\item \(tr(AB-BA)=tr(AB)-tr(BA)=0\). Since \(tr(I_n)=n\), this could not be possible.
\end{enumerate}
\item \textbf{Exercise 12}
\label{sec:org679ad2a}
\(\dim S=\frac{n(n+1)}{2}\). Basis are trivial, skipped.
\item \textbf{Exercise 13}
\label{sec:org8fd5644}
\(tr(AA)=\sum\limits_{i,j=1}^n a_{ij}a_{ji}=\sum\limits_{i,j=1}^n (a_{ij})^2\geq 0\)
\end{enumerate}
\end{document}
