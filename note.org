#+TITLE: Linear Algebra Notes
#+AUTHOR: Harry Ying
#+OPTIONS: date:nil
#+LATEX_HEADER: \hypersetup{colorlinks=true,linkcolor=blue}
#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}

* Chapter 1
** $\S$ 1
*** Notes
A generic vector space $V$ is not a field because there is no definition of $v^{-1}$ for some $v\in V$, fulfilling not the definition of a field.\\
**** *Pg. 4 Proof of $(-1)v=v$*
$$\begin{aligned}
(-1)v+v&=(-1)v+1\cdot v\\
&=(-1+1)v\\
&=v+(-v)
\end{aligned}$$
Thus, $(-1)v=-v$.
**** *Pg. 6 Proof of SP 3*
$$\begin{aligned}
(xA)\cdot B&=\sum\limits_{i=1}^{n}(xa_i) b_{i}\\
&=\sum\limits_{i=1}^{n}x(a_i b_{i})\\
&=x\sum\limits_{i=1}^{n} a_i b_{i}\\
&=x(A\cdot B)\\
A\cdot (xB)&=\sum\limits_{i=1}^{n}a_i (xb_{i})\\
&=\sum\limits_{i=1}^{n}x(a_i b_{i})\\
&=x\sum\limits_{i=1}^{n} a_i b_{i}\\
&=x(A\cdot B)
\end{aligned}$$
**** **Pg. 7**
\\
Upper one:
$$\begin{aligned}
(A+B)^2&=(A+B)\cdot (A+B)\\
&=(A+B)\cdot A+(A+B)\cdot B && \text{Use SP 2}\\
&=A^2+B\cdot A+A\cdot B+B^2 && \text{Use SP 1}\\
\end{aligned}$$
Bottom one:
Since $K$ is a field, all *VS* s regarding summation or product of functions are actually closed on $K$. By applying field axioms, $V$ is then a vector space over $K$.
**** **Pg. 9**
\\
<<U+W>>
Let $a_1=(u_1+w_1),a_2=(u_2+w_2)$. Both of them $\in (U+W)$.\\
Since $U,W$ are subspaces of $V$, $U,W\in V$. Thus, $a_1,a_2 \in V$ as $u_1,w_1,u_2,w_2\in V$, moreover, $(U+W)\subset V$.\\
$a_1+a_2=(u_1+u_2)+(w_1+w_2)\in (U+W)$ \\
$ca_1=c(u_1+w_1)=(cu_1)+(cw_1)\in (U+W)$ \\
Since $O\in U$ and $O\in W$, $O=O+O\in (U+W)$. Thus, $(U+W)$ is a subspace of $V$.
*** Exercises
**** *Exercise 1*
Let $v\in{} V$, $c[v+(-v)]=cv+c(-v)=cv+(-c)v=v\cdot{}0=v\cdot{}(1-1)=v+(-v)=O$
**** *Exercise 2*
Since $c\not = 0$
$$\begin{aligned}
O&=cv+[-(cv)]\\
cv&=cv+[-(cv)]\\
O&=-(cv)\\
\frac{-1}{c}\cdot O &= (-c)v\cdot \frac{-1}{c}\\
\frac{-1}{c}\cdot (v-v) &= v\\
\frac{-1}{c}\cdot v+ \frac{1}{c}\cdot v &= v\\
v\cdot (1-1)&=v\\
v-v&=v\\
O&=v
\end{aligned}$$
**** *Exercise 3*
\\
$\forall g\in V, (g+f)(x) = g(x)+f(x) = f(x)+g(x) = (f+g)(x) \Rightarrow g+f = f+g$.\\
If $O+u = u$, $(O+u)(x) = O(x)+u(x)= u(x)$. Therefore, $O(x)=0$.
**** *Exercise 4*
$$\begin{aligned}
v+w&=O\\
v+w&=v+(-v)\\
w&=-v
\end{aligned}$$
**** *Exercise 5*
$$\begin{aligned}
v+w&=v\\
v+(-v)+w&=v+(-v)\\
O+w&=O
\end{aligned}$$
Since $\forall u, O+u=u$, we have $w=O$.
**** *Exercise 6*
\\
Let $W=\{B| B\cdot A_{1}=O\ \text{and}\ B\cdot A_2=O\}$. Specifically, it is clear that $O\in W$ as $O\cdot A = \sum\limits_{i=1}^{n} b_i a_i=\sum\limits_{i=1}^{n} 0\times a_i=0$.\\
Let $v_1,v_2 \in W$ such that $v_1\cdot A_1=0$, $v_1\cdot A_2=0$, $v_2\cdot A_1=0$, $v_2\cdot A_2=0$. Thus,
$$\begin{aligned}
(v_1+v_2)\cdot A_1&=v_1\cdot A_1+v_2\cdot A_1\\
&=O+O\\
&=O\\
[c(v_1+v_2)]\cdot A_1&=(cv_1+cv_2)\cdot A_1\\
&=(cv_1)\cdot A_1+(cv_2)\cdot A_1\\
&=c(v_1\cdot A_1+v_2\cdot A_1)\\
&=cO\\
&=O
\end{aligned}$$.
It is easy to show for $A_2$ then. Therefore, $(v_1+v_2)\in W$.
**** *Exercise 7*
Same to apply as Exercise 6.
**** *Exercise 8*
\\
Name the set as $W$.
***** Proof
\\
$v_1+v_2=(x_1+x_2,y_1+y_2), x_1+x_2=y_1+y_2 \Rightarrow (v_1+v_2)\in W$ \\
$cv=(cx,cy), cx=cy \Rightarrow cv\in W$ \\
$O=(0,0)\in W$
***** Proof
See Part (a).
***** Proof
Same technique as in Part (a).
**** *Exercise 9*
See Exercise 8.
**** *Exercise 10*
\\
For $U\cap W$, let $v_1,v_2\in U\cap W$. Since $v_1, v_2\in U$ and $U$ is a subspace, $v_1+v_2\in U$. In same way, we can see that $v_{1}+v_2\in W$. Thus, $v_1+v_2\in U\cap W$.\\
Since $v_1\in U$, $cv_1\in U$. Also, it shows $cv_1\in W$ in the same way. Thus, $cv_{1}\in U\cap W$.
Because $U, W$ are subspaces, $O\in U$ and $O\in W$. Thus, $O\in U\cap W$. Therefore, $U\cap W$ is a subspace.\\
Refer to the [[U+W][note part]] for proof for $U+W$.
**** *Exercise 11*
Since $L$ is a field, *VS1, VS3, VS4, VS8* are established under field axioms, and multiplication and addition are closed in $L$. For *VS5, VS6, VS7*, they are all valid as $K\subset L$. $O$ is simply $0$, and $1\cdot u=u$ is  established in $L$.
**** *Exercise 12*
\\
For $x,y\in K$, we have\\
$x+y=a_1+b_1\sqrt{2}+a_2+b_2\sqrt{2}=(a_1+a_2)+(b_1+b_2)\sqrt{2}$. Since $a_1,b_1,a_2,b_2\in \mathbb{Q}$, $(a_1+a_2),(b_1+b_2)\in\mathbb{Q}$. Thus, $x+y\in K$.\\
$xy=(a_1 a_2+ 2b_1 b_2)+(a_2 b_1 + a_1 b_2)\times \sqrt{2}$. Since $a_1,b_1,a_2,b_2\in \mathbb{Q}$, $(a_1 a_2+ 2b_1 b_2),(a_2 b_1 + a_1 b_2)\in\mathbb{Q}$. Thus, $x+y\in K$.\\
$-x=-a+-b\sqrt{2}$. Since $a,b\in\mathbb{Q}$, $-a,-b\in\mathbb{Q}$. Thus, $-x\in K$.\\
If $a+b\sqrt{2}\not = 0$, $a,b\not = 0$, and $a-b\sqrt{2}\not = 0$. Thus, $x^{-1}=\frac{1}{a+b\sqrt{2}}=\frac{a-b\sqrt{2}}{a^2-2b^{2}}=\frac{a}{a^2-2b^2}-\frac{b}{a^2-2b^2}\sqrt{2}$. It is easy to see that *new* $a,b\in\mathbb{Q}$ as $a,b\in\mathbb{Q}$. Thus, $x^{-1}\in K$.
Specifically, if $a=b=0$, $0\in\mathbb{Q}$. If $a=1,b=0$, $1\in\mathbb{Q}$.\\
Thus, $K$ is a field.
**** *Exercise 13*
Same technique as Exercise 12.
**** *Exercise 14*
Same technique as Exercise 12.
** $\S$ 2
*** Notes
<<linear-independence-equivalence>>
Another quite helpful equivalent of definition of linear independence is that (stated following without loss of generality)
$$\forall a_1\not = 0,a_1 v_1\not =\sum\limits_{i=2}^n a_i$$
Here is the /proof/ of equivalence between above statement and definition of linear independence.\\
Since $a_1\not = 0$,
$$\begin{aligned}
v_1&\not =\sum\limits_{i=2}^n \frac{a_i}{a_1}v_i\\
O&\not = -v_1+\sum\limits_{i=2}^n \frac{a_i}{a_1}v_i\\
\lambda O&\not = (-\lambda)v_1 + \sum\limits_{i=2}^n \frac{\lambda a_i}{a_1}v_i && \lambda\in K \text{ and }\lambda\not = 0\\
O&\not = (-\lambda)v_1 + \sum\limits_{i=2}^n \frac{\lambda a_i}{a_1}v_i && \lambda\in K \text{ and }\lambda\not = 0\\
\end{aligned}$$
$\lambda$ and $a_i$ could be arbitrary, thus from above we could conclude that $a'_1 v_1\not =\sum\limits_{i=2}^n a'_i$ if and only if all $a'=0$, which is the definition of linear independence.\\
Also, another point that worth paying attention to is that generators could be *linear dependent*. This is true because you could put arbitrary vectors at the end of a basis of a vector space and just set coefficients for these extraneous vectors when it is producing new linear combinations.
*** Exercises
**** *Exercise 1*
Using result from [[1.2.2.4][*Exercise 4*]], easy to prove.
**** *Exercise 2*
***** $(1,-1)$
***** $(\frac{1}{2},\frac{3}{2})$
***** $(1,1)$
***** $(3,2)$
**** *Exercise 3*
***** $(\frac{1}{3},-\frac{1}{3},\frac{1}{3})$
***** $(1,0,1)$
***** $(\frac{1}{3},-\frac{1}{3},-\frac{2}{3})$
**** *Exercise 4*
<<1.2.2.4>>
\\
Following set of equations is an equivalent of $x(a,b)+y(c,d)=O$,
$$\begin{aligned}
ax+cy&=0 && (1)\\
bx+dy&=0 && (2)\\
\end{aligned}$$
$$\begin{aligned}
(1)\times d-(2)\times c\Rightarrow (ad-cb)x+cdy-cdy &= 0\\
(ad-cb)x&=0\\
\end{aligned}$$
For $ad-cb\not =0$ part, clearly we shall see that $x=0$ as $(ad-cb)x=0$. Plugging $x$ back to $(1)$, we get $y=0$. Thus, two vectors are linear independent.\\
For $ad-cb=0$ part, we need to prove that $x(a,b)+y(c,d)=O$ has solution other than $x=y=0$.\\
First, suppose $a,b,c,d\not = 0$. Since $ad-cb=0$, $x\in \mathbb{R}$. By applying technique, we could also show $y\in \mathbb{R}$. Thus, $(a,b),\ (c,d)$ are linear independent.\\
If $a,b,c,d\not = 0$ does *NOT* hold. Without lose of generality (for all the possibilities, $a,d$ and $c.b$ are interchangeable), consider following scenarios in a $xy$ -plane,
***** $a=0,c=0$
\\
If $a=c=0$, $x,y\in \mathbb{R}$ in $(1)$. Because the $(2)$ is a line in the plane, there must exist some $x,y\not = 0$.
***** $a=0,b=0,c=0$
\\
Same argument as above, despite the line represented by $(2)$ is a little bit peculiar (it is $y=0$).
***** $a=0,d=0,c=0$
\\
Same argument as the first, despite the line represented by $(2)$ is a little bit peculiar (it is $x=0$).
***** $a=0,d=0,b=0,c=0$
\\
Both $(1), (2)$ represent the whole plane, thus, $x,y\in \mathbb{R}$.
**** *Exercise 5,6*
\\
To correctly understand how could functions be elements(vectors) in vector space, we need to understand that function $f:S\rightarrow K$ is essentially a set of pairs $(s,k),\forall s\in S$. Functions have scalar multiplication and addition defined.\\
$f+g$ is defined as $\{(s,f(s)+g(s))|s\in S\}$, and $cf, c\in K$ is defined as $\{(s,c\cdot f(s))|s\in S\}$.\\
It is easy to verify that $V$ of every $f:S \rightarrow K$ is a vector space over $K$. Particularly, $O$ for $V$ is $\{(s,0)|s\in S\}$. So like other vector spaces, linear dependence is *about*
$$f_{sum}=\sum\limits_{i=1}^n a_if_i=O$$
Since right-hand-side of the equation is $\{(s,0)|s\in S\}$, we can say that $\forall v\in V, f_sum (s)=0$. This is useful in solving problems in *Exercise 5* and *Exercise 6*.\\
For example, we need to show that $f(s)=1$ and $g(s)=t$ are linear independent. This means that we need to consider following equation,
$$af+bg=O$$
which is an equivalent of
$$\forall t,a+bt=0$$
Above conversion is quite helpful since we could put in arbitrary $t$ and the equation should holds. Thus, we could put in particular values of $t$ to *construct* set of equations to show that $a=b=0$. For example, here we plug in $t=0$, then $a=0$, and if we plug back $a=0$ into original equation with $t=0$ again, $b=0$.\\
This method could be used throughout *Exercise 5,6*.
**** *Exercise 7*
$(3,5)$
**** *Exercise 8*
/*Calculus involved, not doing now.*/
**** *Exercise 9*
$$\begin{aligned}
\sum\limits_{i=1}^{r} [a_i\cdot (A_i\cdot \sum\limits_{j=i+1}^{r}A_{j})]&=O && \text{All vectors are mutually perpendicular}\\
&=\sum\limits_{i=1}^{r} [(a_i\cdot A_i)\cdot \sum\limits_{j=i+1}^{r}A_{j}]\\
\end{aligned}$$
Since $\forall A\in \{A_i\}, A\not = O$, it is only possible that every $a$ is $0$. Thus, ${A_i}$ are linearly independent.
**** *Exercise 10*
\\
Since $v,w$ are linear dependent, for
$$nv+mw=O$$
at least one of $n,m\not =0$.
Consider following scenarios, we can see that there would be $a=0$ or $a=-\frac{n}{m}$.
***** $n=0,m\not =0 \Rightarrow w=O$
***** $n\not =0,m =0 \Rightarrow v=O$.
This contradicts with $v\not =O$ in problem. Thus, this is impossible.
***** $n\not = 0, m\not =0 \Rightarrow w=\frac{-n}{m}v$
** $\S$ 3
*** Notes
This subsection comprises a lot of concise proofs. But in conclusion, we need to know that
$$\begin{aligned}
\text{Basis}&\Leftrightarrow \text{Maximal linear independent vector set} && \text{proof at }\bold{Theorem 3.1}\\
\text{Basis}&\Leftrightarrow \text{Maximal linear indpendent vector set} \Rightarrow \text{Generators} && \text{proof at }\bold{Theorem 2.2}\\
\text{Generators} &\nRightarrow \text{Basis} && \text{Generators are not always linear independent.}
\end{aligned}$$
Thus, all possible bases of a vector space $V$ are of one and only one possible number of elements, which is equal to the one of maximal independent vector set.
** $\S$ 4
*** Notes
/Proof/ for $$\dim (U\times W)=\dim U+\dim W$$
Because $\forall u\in (U\times W) ,(O_u+O_w)+u=u+(O_u+O_w)=u$. Thus, by definition, $O=(O_u,O_w)$.\\
Let $A=\{u_i\}$ be a basis of $U$ and $B=\{w_i\}$ be a basis of $W$. Note the dimension of $U,W$ as $n,m$ respectively. Let $$C=\{(u_i,0)|u_i\in A\}\cup\{(0,w_i)|w_i\in B\}$$
Since there would be no intersection between two sets being union above, the number of elements in $C$ is $n+m$.
If we could show that $C$ is a basis of $U\times W$, then we could show the original statement.\\
First we need to show that all elements in $C$ is linear independent. This means $a_i\in K,c_i\in C$
$$\sum\limits_{i=1}^{n+m}a_i c_i=O$$
if and only if all the $a_i=0$.\\
Because multiplication by scalar and addition for $U\times W$ is defined componentwise, we shall see that (if we keep the "order" of elements in $C$ as $A$ and $B$ are merged)
$$\begin{aligned}
\sum\limits_{i=1}^{n}a_i u_i&=O_u\\
\sum\limits_{i=n+1}^{n+m}a_i w_i&=O_w
\end{aligned}$$
Since both $A$ and $B$ are basis of $U$ and $W$ respectively, all the $a_i$ should be $0$.\\
Now, we need to show that $C$ generates $U\times W$. Since $A$ and $B$ are basis of $U$ and $W$ respectively,
$$\forall (a,b)\in (U\times W), \exists f_i,g_i\in K:\sum\limits_{i=1}^n f_i u_i=a \text{ and } \sum\limits_{i=1}^m g_i w_i=b $$
Thus, by setting set of scalar for "order"-kept $C$ as $\{f_i\}\cup \{g_i\}$, it is easy to see that it generates $U\times W$.\\
Therefore, we see that
$$\dim (U\times W)=\dim U+\dim W$$
and
$$\{(u_i,0)|u_i\in A\}\cup\{(0,w_i)|w_i\in B\}$$
is a basis for $U\times W$.\\
*** Exercises
**** *Exercise 1*
\\
For the first part, we need to show that $\forall v\in V, \exists \text{ unique } u\in U, w\in W: v=u+w$. Since $(2,1)$ and $(0,1)$ are linear independent, they are a basis of $V=\mathbb{R}^2$. This means
$$\forall v\in V, \exists \text{ unique } a,b\in K: v=a\cdot (2,1)+b\cdot (0,1)$$
Thus, just set $u=a\cdot (2,1)$ and $w=b\cdot (0,1)$, and we have proved it.\\
It is same for $(2,1)$ and $(1,1)$.
**** *Exercise 2*
\\
Since $(1,0,0), (1,1,0), (0,1,1)$ are linear independent, we obtain that
$$\forall v\in V, \exists \text{ unique } a,b,c\in K: v=a\cdot (1,0,0)+b\cdot (1,1,0)+ c\cdot (0,1,1)$$
Set $u=a\cdot (1,0,0)$ and $w=b\cdot (1,1,0)+ c\cdot (0,1,1)$, it would be proved.
**** *Exercise 3*
According to argument provided [[linear-independence-equivalence][here]], $\forall c\in K,cA\not =B$ means that $A,B$ are linear independent. Also, according to *Theorem 3.4*, they are a basis of $\mathbb{R}^2$.\\
Based on the similar argument in *Exercise 1*, second part could be proved.
**** *Exercise 4*
See notes
