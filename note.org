#+TITLE: Linear Algebra Notes
#+AUTHOR: Harry Ying
#+OPTIONS: date:nil
#+LATEX_HEADER: \hypersetup{colorlinks=true,linkcolor=blue}
#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}

* Chapter 1
** $\S$ 1
*** Notes
A generic vector space $V$ is not a field because there is no definition of $v^{-1}$ for some $v\in V$, fulfilling not the definition of a field.\\
**** *Pg. 4 Proof of $(-1)v=v$*
$$\begin{aligned}
(-1)v+v&=(-1)v+1\cdot v\\
&=(-1+1)v\\
&=v+(-v)
\end{aligned}$$
Thus, $(-1)v=-v$.
**** *Pg. 6 Proof of SP 3*
$$\begin{aligned}
(xA)\cdot B&=\sum\limits_{i=1}^{n}(xa_i) b_{i}\\
&=\sum\limits_{i=1}^{n}x(a_i b_{i})\\
&=x\sum\limits_{i=1}^{n} a_i b_{i}\\
&=x(A\cdot B)\\
A\cdot (xB)&=\sum\limits_{i=1}^{n}a_i (xb_{i})\\
&=\sum\limits_{i=1}^{n}x(a_i b_{i})\\
&=x\sum\limits_{i=1}^{n} a_i b_{i}\\
&=x(A\cdot B)
\end{aligned}$$
**** **Pg. 7**
\\
<<(a+b)^2>>
Upper one:
$$\begin{aligned}
(A+B)^2&=(A+B)\cdot (A+B)\\
&=(A+B)\cdot A+(A+B)\cdot B && \text{Use SP 2}\\
&=A^2+B\cdot A+A\cdot B+B^2 && \text{Use SP 1}\\
\end{aligned}$$
Bottom one:
Since $K$ is a field, all *VS* s regarding summation or product of functions are actually closed on $K$. By applying field axioms, $V$ is then a vector space over $K$.
**** **Pg. 9**
\\
<<U+W>>
Let $a_1=(u_1+w_1),a_2=(u_2+w_2)$. Both of them $\in (U+W)$.\\
Since $U,W$ are subspaces of $V$, $U,W\in V$. Thus, $a_1,a_2 \in V$ as $u_1,w_1,u_2,w_2\in V$, moreover, $(U+W)\subset V$.\\
$a_1+a_2=(u_1+u_2)+(w_1+w_2)\in (U+W)$ \\
$ca_1=c(u_1+w_1)=(cu_1)+(cw_1)\in (U+W)$ \\
Since $O\in U$ and $O\in W$, $O=O+O\in (U+W)$. Thus, $(U+W)$ is a subspace of $V$.
*** Exercises
**** *Exercise 1*
Let $v\in{} V$, $c[v+(-v)]=cv+c(-v)=cv+(-c)v=v\cdot{}0=v\cdot{}(1-1)=v+(-v)=O$
**** *Exercise 2*
Since $c\not = 0$
$$\begin{aligned}
O&=cv+[-(cv)]\\
cv&=cv+[-(cv)]\\
O&=-(cv)\\
\frac{-1}{c}\cdot O &= (-c)v\cdot \frac{-1}{c}\\
\frac{-1}{c}\cdot (v-v) &= v\\
\frac{-1}{c}\cdot v+ \frac{1}{c}\cdot v &= v\\
v\cdot (1-1)&=v\\
v-v&=v\\
O&=v
\end{aligned}$$
**** *Exercise 3*
\\
$\forall g\in V, (g+f)(x) = g(x)+f(x) = f(x)+g(x) = (f+g)(x) \Rightarrow g+f = f+g$.\\
If $O+u = u$, $(O+u)(x) = O(x)+u(x)= u(x)$. Therefore, $O(x)=0$.
**** *Exercise 4*
$$\begin{aligned}
v+w&=O\\
v+w&=v+(-v)\\
w&=-v
\end{aligned}$$
**** *Exercise 5*
$$\begin{aligned}
v+w&=v\\
v+(-v)+w&=v+(-v)\\
O+w&=O
\end{aligned}$$
Since $\forall u, O+u=u$, we have $w=O$.
**** *Exercise 6*
\\
Let $W=\{B| B\cdot A_{1}=O\ \text{and}\ B\cdot A_2=O\}$. Specifically, it is clear that $O\in W$ as $O\cdot A = \sum\limits_{i=1}^{n} b_i a_i=\sum\limits_{i=1}^{n} 0\times a_i=0$.\\
Let $v_1,v_2 \in W$ such that $v_1\cdot A_1=0$, $v_1\cdot A_2=0$, $v_2\cdot A_1=0$, $v_2\cdot A_2=0$. Thus,
$$\begin{aligned}
(v_1+v_2)\cdot A_1&=v_1\cdot A_1+v_2\cdot A_1\\
&=O+O\\
&=O\\
[c(v_1+v_2)]\cdot A_1&=(cv_1+cv_2)\cdot A_1\\
&=(cv_1)\cdot A_1+(cv_2)\cdot A_1\\
&=c(v_1\cdot A_1+v_2\cdot A_1)\\
&=cO\\
&=O
\end{aligned}$$.
It is easy to show for $A_2$ then. Therefore, $(v_1+v_2)\in W$.
**** *Exercise 7*
Same to apply as Exercise 6.
**** *Exercise 8*
\\
Name the set as $W$.
***** Proof
\\
$v_1+v_2=(x_1+x_2,y_1+y_2), x_1+x_2=y_1+y_2 \Rightarrow (v_1+v_2)\in W$ \\
$cv=(cx,cy), cx=cy \Rightarrow cv\in W$ \\
$O=(0,0)\in W$
***** Proof
See Part (a).
***** Proof
Same technique as in Part (a).
**** *Exercise 9*
See Exercise 8.
**** *Exercise 10*
\\
For $U\cap W$, let $v_1,v_2\in U\cap W$. Since $v_1, v_2\in U$ and $U$ is a subspace, $v_1+v_2\in U$. In same way, we can see that $v_{1}+v_2\in W$. Thus, $v_1+v_2\in U\cap W$.\\
Since $v_1\in U$, $cv_1\in U$. Also, it shows $cv_1\in W$ in the same way. Thus, $cv_{1}\in U\cap W$.
Because $U, W$ are subspaces, $O\in U$ and $O\in W$. Thus, $O\in U\cap W$. Therefore, $U\cap W$ is a subspace.\\
Refer to the [[U+W][note part]] for proof for $U+W$.
**** *Exercise 11*
Since $L$ is a field, *VS1, VS3, VS4, VS8* are established under field axioms, and multiplication and addition are closed in $L$. For *VS5, VS6, VS7*, they are all valid as $K\subset L$. $O$ is simply $0$, and $1\cdot u=u$ is  established in $L$.
**** *Exercise 12*
\\
For $x,y\in K$, we have\\
$x+y=a_1+b_1\sqrt{2}+a_2+b_2\sqrt{2}=(a_1+a_2)+(b_1+b_2)\sqrt{2}$. Since $a_1,b_1,a_2,b_2\in \mathbb{Q}$, $(a_1+a_2),(b_1+b_2)\in\mathbb{Q}$. Thus, $x+y\in K$.\\
$xy=(a_1 a_2+ 2b_1 b_2)+(a_2 b_1 + a_1 b_2)\times \sqrt{2}$. Since $a_1,b_1,a_2,b_2\in \mathbb{Q}$, $(a_1 a_2+ 2b_1 b_2),(a_2 b_1 + a_1 b_2)\in\mathbb{Q}$. Thus, $x+y\in K$.\\
$-x=-a+-b\sqrt{2}$. Since $a,b\in\mathbb{Q}$, $-a,-b\in\mathbb{Q}$. Thus, $-x\in K$.\\
If $a+b\sqrt{2}\not = 0$, $a,b\not = 0$, and $a-b\sqrt{2}\not = 0$. Thus, $x^{-1}=\frac{1}{a+b\sqrt{2}}=\frac{a-b\sqrt{2}}{a^2-2b^{2}}=\frac{a}{a^2-2b^2}-\frac{b}{a^2-2b^2}\sqrt{2}$. It is easy to see that *new* $a,b\in\mathbb{Q}$ as $a,b\in\mathbb{Q}$. Thus, $x^{-1}\in K$.
Specifically, if $a=b=0$, $0\in\mathbb{Q}$. If $a=1,b=0$, $1\in\mathbb{Q}$.\\
Thus, $K$ is a field.
**** *Exercise 13*
Same technique as Exercise 12.
**** *Exercise 14*
Same technique as Exercise 12.
** $\S$ 2
*** Notes
<<linear-independence-equivalence>>
Another quite helpful equivalent of definition of linear independence is that (stated following without loss of generality)
$$\forall a_i\in K \text{ and some } a_i\not = 0, \text{ we have }a_1 v_1\not =\sum\limits_{i=2}^n a_iv_i$$
Here is the /proof/ of equivalence between above statement and definition of linear independence.\\
$$\begin{aligned}
a_1 v_1&\not =\sum\limits_{i=2}^n a_iv_i\\
O &\not = \sum\limits_{i=1}^n a_iv_i
\end{aligned}$$
This means as long as *some* $a_i\not =0$, $O \not = \sum\limits_{i=1}^n a_iv_i$. In other words, only if all $a_i=0$, $O = \sum\limits_{i=1}^n a_iv_i$. This means any $v_i$ fulfilling our statement are linear independent. Conversely, if $v_i$ are linear independent, it is clear that as long as *not all* $v_i=0$, $a_1 v_1\not =\sum\limits_{i=2}^n a_iv_i$, which is equal to our statement.\\
A simple but useful variation of this is
$$\forall v_i\in K, v_1\not = \sum\limits_{i=2}^n x_iv_i$$
/Proof/. We see that
$$\begin{aligned}
O&\not = -v_1+\sum\limits_{i=2}^n x_iv_i\\
O&\not = (-\lambda)v_1+\sum\limits_{i=2}^n \lambda x_iv_i && \lambda\not = 0\text{ (If } \lambda=0 \text{ inequality holds not)}
\end{aligned}$$
Since $\lambda$ and $v_i$ can be arbitrary and they cannot be $0$ all at once, we see it falls into the case of original statement.\\
Also, another point that worth paying attention to is that generators could be *linear dependent*. This is true because you could put arbitrary vectors at the end of a basis of a vector space and just set coefficients for these extraneous vectors when it is producing new linear combinations.
*** Exercises
**** *Exercise 1*
Using result from [[1.2.2.4][*Exercise 4*]], easy to prove.
**** *Exercise 2*
***** $(1,-1)$
***** $(\frac{1}{2},\frac{3}{2})$
***** $(1,1)$
***** $(3,2)$
**** *Exercise 3*
***** $(\frac{1}{3},-\frac{1}{3},\frac{1}{3})$
***** $(1,0,1)$
***** $(\frac{1}{3},-\frac{1}{3},-\frac{2}{3})$
**** *Exercise 4*
<<1.2.2.4>>
\\
Following set of equations is an equivalent of $x(a,b)+y(c,d)=O$,
$$\begin{aligned}
ax+cy&=0 && (1)\\
bx+dy&=0 && (2)\\
\end{aligned}$$
$$\begin{aligned}
(1)\times d-(2)\times c\Rightarrow (ad-cb)x+cdy-cdy &= 0\\
(ad-cb)x&=0\\
\end{aligned}$$
For $ad-cb\not =0$ part, clearly we shall see that $x=0$ as $(ad-cb)x=0$. Plugging $x$ back to $(1)$, we get $y=0$. Thus, two vectors are linear independent.\\
For $ad-cb=0$ part, we need to prove that $x(a,b)+y(c,d)=O$ has solution other than $x=y=0$.\\
First, suppose $a,b,c,d\not = 0$. Since $ad-cb=0$, $x\in \mathbb{R}$. By applying technique, we could also show $y\in \mathbb{R}$. Thus, $(a,b),\ (c,d)$ are linear independent.\\
If $a,b,c,d\not = 0$ does *NOT* hold. Without lose of generality (for all the possibilities, $a,d$ and $c.b$ are interchangeable), consider following scenarios in a $xy$ -plane,
***** $a=0,c=0$
\\
If $a=c=0$, $x,y\in \mathbb{R}$ in $(1)$. Because the $(2)$ is a line in the plane, there must exist some $x,y\not = 0$.
***** $a=0,b=0,c=0$
\\
Same argument as above, despite the line represented by $(2)$ is a little bit peculiar (it is $y=0$).
***** $a=0,d=0,c=0$
\\
Same argument as the first, despite the line represented by $(2)$ is a little bit peculiar (it is $x=0$).
***** $a=0,d=0,b=0,c=0$
\\
Both $(1), (2)$ represent the whole plane, thus, $x,y\in \mathbb{R}$.
**** *Exercise 5,6*
\\
To correctly understand how could functions be elements(vectors) in vector space, we need to understand that function $f:S\rightarrow K$ is essentially a set of pairs $(s,k),\forall s\in S$. Functions have scalar multiplication and addition defined.\\
$f+g$ is defined as $\{(s,f(s)+g(s))|s\in S\}$, and $cf, c\in K$ is defined as $\{(s,c\cdot f(s))|s\in S\}$.\\
It is easy to verify that $V$ of every $f:S \rightarrow K$ is a vector space over $K$. Particularly, $O$ for $V$ is $\{(s,0)|s\in S\}$. So like other vector spaces, linear dependence is *about*
$$f_{sum}=\sum\limits_{i=1}^n a_if_i=O$$
Since right-hand-side of the equation is $\{(s,0)|s\in S\}$, we can say that $\forall v\in V, f_sum (s)=0$. This is useful in solving problems in *Exercise 5* and *Exercise 6*.\\
For example, we need to show that $f(s)=1$ and $g(s)=t$ are linear independent. This means that we need to consider following equation,
$$af+bg=O$$
which is an equivalent of
$$\forall t,a+bt=0$$
Above conversion is quite helpful since we could put in arbitrary $t$ and the equation should hold. Thus, we could put in particular values of $t$ to *construct* set of equations to show that $a=b=0$. For example, here we plug in $t=0$, then $a=0$, and if we plug back $a=0$ into original equation with $t=0$ again, $b=0$.\\
This method could be used throughout *Exercise 5,6*.
**** *Exercise 7*
$(3,5)$
**** *Exercise 8*
/*Calculus involved, not doing now.*/
**** *Exercise 9*
$$\begin{aligned}
\sum\limits_{i=1}^{r} [a_i\cdot (A_i\cdot \sum\limits_{j=i+1}^{r}A_{j})]&=O && \text{All vectors are mutually perpendicular}\\
&=\sum\limits_{i=1}^{r} [(a_i\cdot A_i)\cdot \sum\limits_{j=i+1}^{r}A_{j}]\\
\end{aligned}$$
Since $\forall A\in \{A_i\}, A\not = O$, it is only possible that every $a$ is $0$. Thus, ${A_i}$ are linearly independent.
**** *Exercise 10*
\\
Since $v,w$ are linear dependent, for
$$nv+mw=O$$
at least one of $n,m\not =0$.
Consider following scenarios, we can see that there would be $a=0$ or $a=-\frac{n}{m}$.
***** $n=0,m\not =0 \Rightarrow w=O$
***** $n\not =0,m =0 \Rightarrow v=O$.
This contradicts with $v\not =O$ in problem. Thus, this is impossible.
***** $n\not = 0, m\not =0 \Rightarrow w=\frac{-n}{m}v$
** $\S$ 3
*** Notes
This subsection comprises a lot of concise proofs. But in conclusion, we need to know that
$$\begin{aligned}
\text{Basis}&\Leftrightarrow \text{Maximal linear independent vector set} && \text{proof at }\bold{Theorem 3.1}\\
\text{Basis}&\Leftrightarrow \text{Maximal linear indpendent vector set} \Rightarrow \text{Generators} && \text{proof at }\bold{Theorem 2.2}\\
\text{Generators} &\nRightarrow \text{Basis} && \text{Generators are not always linear independent.}
\end{aligned}$$
Thus, all possible bases of a vector space $V$ are of one and only one possible number of elements, which is equal to the one of maximal independent vector set.
** $\S$ 4
*** Notes
/Proof/ for $$\dim (U\times W)=\dim U+\dim W$$
Because $\forall u\in (U\times W) ,(O_u+O_w)+u=u+(O_u+O_w)=u$. Thus, by definition, $O=(O_u,O_w)$.\\
Let $A=\{u_i\}$ be a basis of $U$ and $B=\{w_i\}$ be a basis of $W$. Note the dimension of $U,W$ as $n,m$ respectively. Let $$C=\{(u_i,0)|u_i\in A\}\cup\{(0,w_i)|w_i\in B\}$$
Since there would be no intersection between two sets being union above, the number of elements in $C$ is $n+m$.
If we could show that $C$ is a basis of $U\times W$, then we could show the original statement.\\
First we need to show that all elements in $C$ is linear independent. This means $a_i\in K,c_i\in C$
$$\sum\limits_{i=1}^{n+m}a_i c_i=O$$
if and only if all the $a_i=0$.\\
Because multiplication by scalar and addition for $U\times W$ is defined componentwise, we shall see that (if we keep the "order" of elements in $C$ as $A$ and $B$ are merged)
$$\begin{aligned}
\sum\limits_{i=1}^{n}a_i u_i&=O_u\\
\sum\limits_{i=n+1}^{n+m}a_i w_i&=O_w
\end{aligned}$$
Since both $A$ and $B$ are basis of $U$ and $W$ respectively, all the $a_i$ should be $0$.\\
Now, we need to show that $C$ generates $U\times W$. Since $A$ and $B$ are basis of $U$ and $W$ respectively,
$$\forall (a,b)\in (U\times W), \exists f_i,g_i\in K:\sum\limits_{i=1}^n f_i u_i=a \text{ and } \sum\limits_{i=1}^m g_i w_i=b $$
Thus, by setting set of scalar for "order"-kept $C$ as $\{f_i\}\cup \{g_i\}$, it is easy to see that it generates $U\times W$.\\
Therefore, we see that
$$\dim (U\times W)=\dim U+\dim W$$
and
$$\{(u_i,0)|u_i\in A\}\cup\{(0,w_i)|w_i\in B\}$$
is a basis for $U\times W$.\\
*** Exercises
**** *Exercise 1*
\\
For the first part, we need to show that $\forall v\in V, \exists \text{ unique } u\in U, w\in W: v=u+w$. Since $(2,1)$ and $(0,1)$ are linear independent, they are a basis of $V=\mathbb{R}^2$. This means
$$\forall v\in V, \exists \text{ unique } a,b\in K: v=a\cdot (2,1)+b\cdot (0,1)$$
Thus, just set $u=a\cdot (2,1)$ and $w=b\cdot (0,1)$, and we have proved it.\\
It is same for $(2,1)$ and $(1,1)$.
**** *Exercise 2*
\\
Since $(1,0,0), (1,1,0), (0,1,1)$ are linear independent, we obtain that
$$\forall v\in V, \exists \text{ unique } a,b,c\in K: v=a\cdot (1,0,0)+b\cdot (1,1,0)+ c\cdot (0,1,1)$$
Set $u=a\cdot (1,0,0)$ and $w=b\cdot (1,1,0)+ c\cdot (0,1,1)$, it would be proved.
**** *Exercise 3*
$$\begin{aligned}
cA&\not =B\\
O&\not = B-cA\\
O&\not =\lambda B-c\lambda A && \lambda\not = 0\text{ (If } \lambda=0 \text{ inequality holds not)}
\end{aligned}$$
Since $\lambda ,c$ are arbitrary and $\lambda\not = 0$, coefficients before $A$ and $B$ can be anything but not equal to $0$ together.
According to argument provided [[linear-independence-equivalence][here]], $A,B$ are linear independent. Also, according to *Theorem 3.4*, they are a basis of $\mathbb{R}^2$.\\
Based on the similar argument in *Exercise 1*, second part could be proved.
**** *Exercise 4*
See notes
* Chapter 2
** $\S$ 1
*** Exercises
**** *Exercise 1*
Skip
**** *Exercise 2*
Skip
**** *Exercise 3*
Skip
**** *Exercise 4*
Skip
**** *Exercise 5*
Let $C=^t(A+B)=(c_{ij})$. Then, $c_{ij}=(a_{ij}+b_{ij})'=a_{ji}+b_{ji}$. Thus, $C=^tA+^tB$.
**** *Exercise 6*
Let $B=^t(cA)$. Then, $b_{ij}=ca_{ji}$. Since $^tA=(a^{\prime}_{ij})=(a_{ji})=A$, $B=c^tA$.
**** *Exercise 7*
No difference.
**** *Exercise 8*
Skip
**** *Exercise 9*
Skip
**** *Exercise 10*
Let $B=A+^tA=(b_{ij})=(a_{ij}+a_{ji})$. Since, $b_{ij}=a_{ij}+a_{ji}=a_{ji}+b_{ij}=b_{ji}$, $B$ is symmetric.
**** *Exercise 11*
Skip
**** *Exercise 12*
Skip
**** *Exercise 13*
\\
*For followings, we mean ones in /Exercises on Dimension/ section*.\\
Followings are linear independent.
$$U_1=\begin{pmatrix}
   1 & 0 \\
   0 & 0
\end{pmatrix}$$
$$U_2=\begin{pmatrix}
   0 & 1 \\
   0 & 0
\end{pmatrix}$$
$$U_3=\begin{pmatrix}
   0 & 0 \\
   1 & 0
\end{pmatrix}$$
$$U_4=\begin{pmatrix}
   0 & 0 \\
   0 & 1
\end{pmatrix}$$
Apply $a\cdot U_1+b\cdot U_2+c\cdot U_3+d\cdot U_4=O$ to verify it.
Because it generates the matrix vector space $Mat_{2\times 2}K$ over K (For every $v\in Mat_{2\times 2}K$, simply let $a,b,c,d$ be $v$'s components) and $\{U_i\}$ are linear independent, $\{U_i\}$ is a basis of $Mat_{2\times 2}K$.\\
Because the number of elements in a basis is the dimension of the vector space, we see that the dimension of it is 4.
**** *Exercise 14*
Similar argument to *Exercise 13*. Dimension of it is $mn$.
**** *Exercise 15*
Dimension of it is $n$. Simply build up a basis to see.
**** *Exercise 16*
Similarly, dimension of it is $\frac{(n+1)n}{2}$.
**** *Exercise 17*
\\
Basis is a set comprises
$$U_1=\begin{pmatrix}
   1 & 0 \\
   0 & 0
\end{pmatrix}$$
$$U_2=\begin{pmatrix}
   0 & 1 \\
   1 & 0
\end{pmatrix}$$
$$U_3=\begin{pmatrix}
   0 & 0 \\
   0 & 1
\end{pmatrix}$$
Then, it is easy to see that dimension is 3.
**** *Exercise 18*
Basis similar to the one in *Exercise 17* is linear independent and generates space. And, indeed, the number of elements in the basis is the same as one in *Exercise 16*. Thus, dimension of it is $\frac{n(n+1)}{2}$.
**** *Exercise 19*
Same as *Exercise 15*.
**** *Exercise 20*
\\
Let $U$ be the subspace of $V$. There would be a maximal number $m$ of linear independent vectors (*Theorem 3.1* in chapter 1). Suppose the number $m> \dim V$. Then it would contradicts *Theorem 3.1* in chapter 1 as any number of vectors more than $\dim V$ would be linear dependent, which means the basis of $U$ would be linear dependent (remember $U$ is a subspace of $V$). Thus, $m\leq \dim V$.\\
Dimension could be $0,1,2$.
**** *Exercise 21*
\\
According to the lemma we proved in *Exercise 20*, dimension of subspace of $\mathbb{R}^{3}$ could be $0,1,2,3$.
** $\S$ 2
*** Notes
*Lemma* Let $A$ be a set of linear dependent vectors that generates $V$. Then, for all $v\in V$, there exists infinite linear combinations of $A$ that form $v$.\\
/Proof/ Say that number of vectors in $A$ is $n$. Since $A$ generates $V$, $\forall v\in V, \exists \{a_i\}: v=\sum\limits_{i=1}^n a_i A_i$. Let $L$ be a set of linear combinations that form $v$ (here $L$ is a set of sets). We have
$$\begin{aligned}
v&=\sum\limits_{i=1}^n a_iA_i+O\\
&=\sum\limits_{i=1}^n a_iA_i+\sum\limits_{i=1}^n b_iA_i\\
&=\sum\limits_{i=1}^n (a_i+b_i) A_i
\end{aligned}$$
Since $A$ is linear dependent, there exists $\{b_i\}$ where not every element is $0$. Therefore, $\{a_i+b_i\}\in L$ and $\{a_i+b_i\}\not = \{a_i\}$ for some $\{b_i\}$.\\
This means that $\forall \ell \in L$, we can always form a new $\ell^{\prime}\in L$. And since for all $v\in V$ we always have one linear combination, we can do it infinitely, which means number of elements in $L$ is infinite. Therefore, we have shown what was to be shown. $\blacksquare$ \\
Here we discuss the number of solutions for general linear equations. ($A$ is a $m\times n$ matrix. $X$ is a $n\times 1$ column matrix. $B$ is a $m\times 1$ column matrix).
$$AX=B$$
If $n>m$, according to *Theorem 3.1 in chapter 1*, they must be linear dependent, resulting in infinite number of solutions because of *Lemma* above.\\
If $n=m$ and they are linear independent (it is then a basis because they are maximal independent vectors), there would only be one solution as *Theorem 2.1 in chapter 1* stated. If they are linear dependent and $B$ is in the subspace generated by column vectors of $A$, there would be infinite number of solutions (*Lemma*), else the equations are not solvable (there exists no linear combination to represent $B$).\\
If $n<m$ and they are independent and $B$ is in the subspace generated by column vectors of $A$, there would be only one solution. If they are linear independent but $B$ is not in subspace, then it is unsolvable. If they are linear dependent and $B$ is in subspace, infinite solutions occur. If they are linear dependent but $B$ is not in subspace, equations are not solvable.\\
In general,
***** If $B$ is in the vector space generated by column vectors of $A$ and they are linear independent, there exists one unique solution.
***** If $B$ is in the vector space generated by column vectors of $A$ and they are linear dependent, there exists infinite solutions.
***** If $B$ is not in the vector space generated by column vectors of $A$, there would be no solution.
*** Exercises
**** *Exercise 1*
See notes and refer to the definition of linear independence.
**** *Exercise 2*
\\
Let $u$ be one set of solution and $w$ be another.\\
We want to show that $u+w\in X$.
$$\sum\limits_{i=1}^n (u_i+w_i)\cdot A^i=\sum\limits_{i=1}^n u_i\cdot A^i+\sum\limits_{i=1}^n w_i\cdot A^i=O+O=O$$
Thus, $u+w\in X$.
Also, we need to show $cu\in X$ where $c\in K$.
$$c \sum\limits_{i=1}^n u_i \cdot A^i=cO=O$$
Other *VS* s are easy to follow as we define the addition of vectors in $X$ componentwise, $O$ as a vector whose components are all zero, $1$ as a vector whose components are all one.
**** *Exercise 3*
We want to show following
$$\begin{aligned}
\sum\limits_{i=1}^n (a_i+b_i \text{i}) A^i&=O_{\mathbb{C}}\\
\sum\limits_{i=1}^n a_iA^{i}+\sum\limits_{i=1}^n b_i \text{i} \cdot A^i &= O_{\mathbb{C}}\\
O_{\mathbb{C}}+\sum\limits_{i=1}^n b_i \text{i} \cdot A^i&=O_{\mathbb{C}}\\
\sum\limits_{i=1}^n b_i \cdot A^i&=O_{\mathbb{C}}
\end{aligned}$$
This means that $\{A^i\}$ should be linear independent over $\mathbb{R}$ ($\sum\limits_{i=1}^n b_i \cdot A^i=O_{\mathbb{C}}$ is equal to $\sum\limits_{i=1}^n b_i \cdot A^i=O_{\mathbb{R}}$ as there is no imaginary part). Since it is known to us that $\{A^i\}$ is linear independent over $\mathbb{R}$, it has been proved as we do it reversely.
**** *Exercise 4*
We know that
$$\sum\limits_{i=1}^n (a_i+b_i \text{i}) A^i = O_{\mathbb{C}}$$
which means that $\sum\limits_{i=1}^n a_i A^i = O_{\mathbb{C}}$ and/or $\sum\limits_{i=1}^n b_i A^i = O_{\mathbb{C}}$.
For either cases, we have shown it is linear dependent over $\mathbb{R}$ ($a_i,b_i\in \mathbb{R}$).
** $\S$ 3
*** Exercises
**** *Exercise 1*
$AI=IA=A$
**** *Exercise 2*
$AO=O$
**** *Exercise 3*
\\
For every $A$ and $B$, $(AB)C=A(BC)$.
***** Case 1
$$\begin{pmatrix}
3&2\\
4&1
\end{pmatrix}$$
***** Case 2
$$\begin{pmatrix}
10\\
14
\end{pmatrix}$$
***** Case 3
$$\begin{pmatrix}
33&37\\
11&-18
\end{pmatrix}$$
**** *Exercise 4*
This one could be proved as it is proved [[(a+b)^2][here]].
**** *Exercise 5*
$$AB=\begin{pmatrix}
4&2\\
5&-1
\end{pmatrix}$$
$$BA=\begin{pmatrix}
2&4\\
4&1
\end{pmatrix}$$
**** *Exercise 6*
$$CA=AC=\begin{pmatrix}
7&14\\
21&-7
\end{pmatrix}$$
$$CB=BC=\begin{pmatrix}
14&0\\
7&7
\end{pmatrix}$$
General rule is that for symmetric one, we may have $AB=BA$? (I am not sure here).
**** *Exercise 7*
$$XA=\begin{pmatrix}
3&1&5
\end{pmatrix}$$
**** *Exercise 8*
$$\begin{aligned}
X_1 A=A_2\\
X_2 A=A_3
\end{aligned}$$
Let $X_i$ be a unit vector with only $i$ -th component equal to $1$. $X_i A=A_i$
**** *Exercise 9*
\\
Skip the steps involving verifications.
$^t (AB)=^tB^tA$ has already been proved in $\S2$. Thus, $^t [(AB)C]=^t C\cdot ^t (AB)=^t C\cdot ^t B\cdot ^tA$.
**** *Exercise 10*
\\
Firstly, we know $A$ is of $1\times n$, $M$ is of $n\times n$ and $B$ is of $1\times n$. This means that $\dim (\langle A,B\rangle)=1$. Also, it implies that $^t (\langle A,B\rangle)=\langle A,B\rangle$. Thus, we have
$$\begin{aligned}
\langle A,B\rangle&=^t (\langle A,B\rangle)\\
&=^t(AM^tB)\\
&=^t(^tB)\cdot ^tM\cdot ^tA && \text{Exercise 9}\\
&=BM^tA\\
&=\langle B,A\rangle
\end{aligned}$$
which is *SP 1*.
Also, let
$$N=^t(B+C)$$
Then, $n_{ij}=n^{\prime}_{ji}=b_{ji}+c_{ji}$. This implies also $N=^tA+^tB$. Therefore,
$$\langle A,B+C\rangle=AM^t(B+C)=AM(^tB+^tC)=\langle A,B\rangle+\langle A,C\rangle$$
which is *SP 2*. Finally
$$\langle cA,B\rangle=cAM^tB=c\langle A,B\rangle$$
which is *SP 3*.
**** *Exercise 11*
\\
For part *(a)*, see *Exercise 35*.\\
Part *(b)*
$$A^2=\begin{pmatrix}
1&2&3\\
0&1&2\\
0&0&1
\end{pmatrix}$$
$$A^3=\begin{pmatrix}
1&3&6\\
0&1&3\\
0&0&1
\end{pmatrix}$$
$$A^4=\begin{pmatrix}
1&4&10\\
0&1&4\\
0&0&1
\end{pmatrix}$$
**** *Exercise 12*
$$(AX)_{a}=\begin{pmatrix}
4\\
7\\
5
\end{pmatrix}
(AX)_{b}=\begin{pmatrix}
3\\
1
\end{pmatrix}$$
$$(AX)_c=\begin{pmatrix}
x_2\\
0
\end{pmatrix}
(AX)_d=\begin{pmatrix}
0\\
x_1
\end{pmatrix}$$
**** *Exercise 13*
$$(AX)_a=\begin{pmatrix}
2\\
4
\end{pmatrix}$$
$$(AX)_b=\begin{pmatrix}
4\\
6
\end{pmatrix}$$
$$(AX)_c=\begin{pmatrix}
3\\
5
\end{pmatrix}$$
**** *Exercise 14*
$$(AX)_a=\begin{pmatrix}
3\\
1\\
2
\end{pmatrix}$$
$$(AX)_b=\begin{pmatrix}
12\\
3\\
9
\end{pmatrix}$$
$$(AX)_c=\begin{pmatrix}
5\\
4\\
8
\end{pmatrix}$$
**** *Exercise 15*
$AX=A^2$ (second column of $A$).
**** *Exercise 16*
$AX=A^i$
**** *Exercise 17*
\\
Let $U_i$ be a unit column vector which only has $1$ on its $i$ -th component. The proposed form of $C^k$ could be written in the following way.
$$\begin{aligned}
C^k&=\sum\limits_{i=1}^n b_{ik}A^i\\
&=\sum\limits_{i=1}^n b_{ik}[\sum\limits_{j=1}^m (a_{ji}\cdot U_j)]\\
&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^m a_{ji}b_{ik}\cdot U_j]\\
\end{aligned}$$
$$\begin{aligned}
C^k&=\sum\limits_{j=1}^m A_j\cdot B^k\cdot U_j\\
&=\sum\limits_{j=1}^m [\sum\limits_{i=1}^n a_{ji}b_{ik}\cdot U_j]
\end{aligned}$$
Two forms are essentially the same if you expand them and compare. Thus, we have proved that the proposed formula is an equivalence of the original definition.
**** *Exercise 18*
***** $A^{-1}=(I+A)\Rightarrow A\cdot A^{-1}=I^2-A^2=I$
***** $A^{-1}=(I^2+IA+A^2)\Rightarrow A\cdot A^{-1}=I^3-A^3=I$
***** For real number $I$ and $A$, we see that $I^n-A^n$ can be factored into $I-A$ and another polynomial, because according to remainder theorem, plugging in $I=A$ results in $I^n-A^n=0$. Thus, we could follow the same pattern to construct always a $A^{-1}$.
***** Set $A^{-1}=(-A-2I)$
***** Set $A^{-1}=(-A^2-A)$
**** *Exercise 19*
$$AB=\begin{pmatrix}
1 & ab\\
0 & 1
\end{pmatrix}$$
$$A^2=\begin{pmatrix}
1 & 2a\\
0 & 1
\end{pmatrix}$$
Inductive step:
$$\begin{aligned}
A^{n+1}&=A^n\cdot A\\
&=\begin{pmatrix}
1 & na\\
0 & 1
\end{pmatrix}\cdot A\\
&=\begin{pmatrix}
1 & (n+1)a\\
0 & 1
\end{pmatrix}
\end{aligned}$$
Thus, we have proved it.
**** *Exercise 20*
$$A^{-1}=\begin{pmatrix}
1 & -a \\
0 & 1
\end{pmatrix}$$
**** *Exercise 21*
We now show that $B^{-1}A^{-1}$ would be an inverse of $AB$.
$$(AB)(B^{-1}A^{-1})=A(B\cdot B^{-1})A^{-1}=A\cdot A^{-1}=I$$
And for the reverse, it is easy to verify either.
**** *Exercise 22*
See the solution manual
**** *Exercise 23*
$$\begin{aligned}
A^2&=A\cdot A\\
&=\begin{pmatrix}
\cos 2\theta & -\sin 2\theta\\
\sin 2\theta & \cos 2\theta
\end{pmatrix}
\end{aligned}$$
Inductive step:
$$\begin{aligned}
A^{n+1}&=\begin{pmatrix}
\cos n\theta & -\sin n\theta\\
\sin n\theta & \cos n\theta
\end{pmatrix}\cdot A\\
&=\begin{pmatrix}
\cos n\theta \cos \theta - \sin n\theta \sin \theta & -(\sin n\theta \cos \theta + \sin \theta \cos n\theta) \\
\sin n\theta \cos \theta + \sin \theta \cos n\theta & - \sin n\theta + \cos n\theta \cos \theta
\end{pmatrix}\\
&=\begin{pmatrix}
\cos (n+1)\theta & -\sin (n+1)\theta\\
\sin (n+1)\theta & \cos (n+1)\theta
\end{pmatrix}
\end{aligned}$$
Thus, we have determined $A^n$
**** *Exercise 24*
$$A=\begin{pmatrix}
0 & 1\\
-1 & 0
\end{pmatrix}$$
**** *Exercise 25*
***** $\text{tr}(A)=2$
***** $\text{tr}(A)=4$
***** $\text{tr}(A)=8$
**** *Exercise 26*
See *Exercise 27*.
**** *Exercise 27*
$$\begin{aligned}
\text{tr}(AB)&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^n a_{ij} b_{ji}]\\
&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^n b_{ji} a_{ij}]\\
&=\sum\limits_{i=1}^n [\sum\limits_{j=1}^n b_{ij} a_{ji}] && \text{They are the same if you expand}\\
&=\text{tr}(BA)
\end{aligned}$$
**** *Exercise 28*
As diagonal line keeps same after transpose, trace of the matrix would not change as well.
**** *Exercise 29*
$A^n=((a_{ij})^n)$
**** *Exercise 30*
$$A^2=\begin{pmatrix}
a_1^2 & 0 & \cdots & 0\\
0 & a_2^2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n^2
\end{pmatrix}$$
Inductive step
$$A^{k+1}=\begin{pmatrix}
a_1^k & 0 & \cdots & 0\\
0 & a_2^k & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n^k
\end{pmatrix}\cdot A=\begin{pmatrix}
a_1^{k+1} & 0 & \cdots & 0\\
0 & a_2^{k+1} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_n^{k+1}
\end{pmatrix}$$
$\blacksquare$
**** *Exercise 31*
See *Exercise 35*
**** *Exercise 32*
We want to show
$$\begin{aligned}
^t(A^{-1})&=(^t A)^{-1}\\
^t(A^{-1})\cdot ^t (A)&=(^t A)^{-1} \cdot (^tA)\\
^t(A^{-1})\cdot ^t (A)&=I_n\\
\end{aligned}$$
Let $C=^t(A^{-1})\cdot ^t (A)$. We then know
$$\begin{aligned}
c_{ij}&=\sum\limits_{k=1}^n a^{\prime -1}_{ik} a^{\prime}_{kj}\\
&=\sum\limits_{k=1}^n a_{jk} a^{-1}_{ki}\\
&=A_j\cdot A^{-1\ i}
\end{aligned}$$
Thus,
$$\begin{aligned}
C&=^t(A\cdot A^{-1})\\
&=^t(I_n)=I_n
\end{aligned}$$
If we do it in the reverse way, then we can prove it.
**** *Exercise 33*
Let $B=^t (\bar{A})$, then $b_{ij}=\bar{a}_{ji}$. Let $C=\overline{^t A}$, then $c_{ij}=\bar{a^{\prime}}_{ij}=\bar{a}_{ji}$. Thus, $B=C$.
**** *Exercise 34*
Its inverse is
$$\begin{pmatrix}
\frac{1}{a_1} & 0 & \cdots & 0\\
0 & \frac{1}{a_2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \frac{1}{a_n}
\end{pmatrix}$$
**** *Exercise 35*
See solution manual. Here I would not like to introduce complex formal reasoning to simulate computation result.
**** *Exercise 36*
\\
By result of *Exercise 35* we see that $N^{n+1}=O$ as $N=A-I_n$ is of the form being described in *Exercise 35*.\\
For inverse part, see *Exercise 18*.
**** *Exercise 37*
$$(I-N)(I+N+\cdot +N^r)=I^{r+1}-N^{r+1}=I^{r+1}=I$$
**** *Exercise 38*
See solution manual for detail computation.
**** *Exercise 39*
Since we know $AB=BA$ or $A,B$ fulfills *SP 1*, we may say
$$(AB)^r=A^rB^r=O$$
For $(A+B)$, we discuss $(A+B)^{2r}$ where $r$ is the larger $r$ for $A$ and $B$.
$$(A+B)^{2r}=\sum _{k=0}^{2r}{\binom {2r}{k}}A^{2r-k}B^{k}$$
If $1\geq k\leq r$, then $2r-k\geq r$ and $A^{2r-k}=O$. If $r<k \leq 2r$, then $B^k=O$. Thus, essentially, $(A+B)^{2r}=O$.
* Chapter 3
** $\S$ 1
*** Notes
If we want to say that $S$ is the image of $A$ under $F$, we are essentially trying to say followings:
$$\begin{aligned}
&\forall z\in S, \exists x:F(x)=z. \Rightarrow S\subset F(A)\\
&\forall a\in A, F(a)\in S. \Rightarrow F(A)\subset S
\end{aligned}$$
Above are exactly what *Example 6* on Pg. 45 are saying.\\
Also, we shall work on the equality of two linear mappings. Two linear mappings $F:S_1\rightarrow T_1 ,G:S_2\rightarrow T_2$ are said to be equal if and only if followings are fulfilled:
$$\begin{aligned}
S_1&=S_2\\
T_1&=T_2\\
\forall z\in S_1, F(z)&=G(z)
\end{aligned}$$
Proofs left to readers on Pg. 49.\\
 /If $u_1,u_2$ are elements of $V$, then $T_{u_1+u_2}=T_{u_1}\circ T_{u_2}$./\\
$$\begin{aligned}
\forall v\in V, T_{u_1+u_2}&=(u_1+u_2)+v\\
&=u_1+(u_2+v)\\
&=T_{u_1}(u_2+v)\\
&=T_{u_1}(T_{u_2}(v))\\
&=T_{u_1}\circ T_{u_2}(v)
\end{aligned}$$
Which means that $T_{u_1+u_2}=T_{u_1}\circ T_{u_2}$ according to our definition of linear mapping equality.\\
/If $u$ is an element of $V$, then $T_u:V\rightarrow V$ has an inverse mapping which is nothing but the translation $T_{-u}$./\\
First, it is easy to verify that $T_{-u}$ is an inverse of $T_u$. Then, we say that there is an inverse $T_u^{-1}$. According to the definition of inverse of a linear mapping, we have, for every $v\in V$ that
$$\begin{aligned}
T_u^{-1}(T_u(v))&=I_V(v)=v\\
T_u^{-1}(v+u)&=v\\
T_u^{-1}(x)&=x-u && \text{Let } x=v+u\\
\end{aligned}$$
which attests $T_{u}^{-1}=T_{-u}$.\\
Here comes words on bijectivity, inverse and function composition:\\
1. For two mappings $F:S_1\rightarrow T_1$ and $F:S_2\rightarrow T_2$, $F\circ G$ is only defined if $T_1=S_2$.\\
2. A more clear proof for /If $F:S\rightarrow V$ has an inverse $G:V\rightarrow S$, then $F$ is bijective./ /Proof./ If $F(x)=F(y)$ given $x,y\in S$, then $G(F(x))=G(F(y))$. Also, since $F,G$ are inverse of each other, we have
$$\begin{aligned}
&\forall s\in S, (G\circ F)(s)=G(F(s))=I_s(s)=s\\
&\forall v\in V, (F\circ G)(v)=F(G(v))=I_v(v)=v
\end{aligned}$$
which means $x=G(F(x))=G(F(y))=y$. Also, we contend that $\forall v\in V, \exists x: F(x)=v$. Since we know $\forall v\in V, (F\circ G)(v)=F(G(v))=I_v(v)=v$, we can simply let $x=G(v)$ so that $F(x)=v$. This proves the theorem.
*** Exercises
**** *Exercise 1*
/*Calculus involved, not doing now.*/
**** *Exercise 2*
Proved in notes.
**** *Exercise 3*
***** $L(X)=11$
***** $L(X)=13$
***** $L(X)=6$
**** *Exercise 4*
\\
$F(1)=(e,1)$, $F(0)=(1,0)$, $F(-1)=(e^{-1},-1)$
**** *Exercise 5*
\\
$(F+G)(1)=(e+1,3)$, $(F+G)(2)=(e^2+2,6)$, $(F+G)(0)=(1,0)$
**** *Exercise 6*
\\
$(2F)(0)=(2,0)$, $(\pi F)(1)=(\pi e,\pi)$
**** *Exercise 7*
For (a), it is 1. For (b), it is 11.
**** *Exercise 8*
\\
The image is a ellipse of the equation
$$\frac{u^2}{4}+\frac{w^2}{9}=1$$
Proof is omitted.
**** *Exercise 9*
\\
The image is a straight line
$$y=\frac{1}{2}x$$
/Proof./ $A=\{(2,y)|y\in \mathbb{R}\}$, $S=\{(2x,x)|x\in \mathbb{R}\}$. We contend that $\forall a\in A, F(a)\in S$.
$$\forall y\in \mathbb{R}, F(2,y)=(2y,y)\in S$$
Conversely, $\forall s=(x,\frac{1}{2}x)\in S$, let $a=(2,\frac{1}{2}x)\in A$, so that $F(a)=s$, which means $S\subset F(A)$.
**** *Exercise 10*
It is a circle of center $(0,0)$ and radius $e^c$. Proof is omitted.
**** *Exercise 11*
\\
It is a cylinder of radius 1 and center $(0,0)$. Proof is omitted.
**** *Exercise 12*
$x^2+y^2=1$. Proof is omitted.
** $\S$ 2
*** Notes
<<lemma_3.2.1>>
Here we have an important *Lemma*\\
/Let $F:V\rightarrow W$ is a linear mapping. If for some $v_i\in V$, we have $F(v_i)$ are linear independent, then $v_i$ are linear independent./\\
/Proof./ If $\sum\limits_{i=1}^n t_i v_i=O$, then we have
$$\begin{aligned}
\sum\limits_{i=1}^n t_i v_i&=O\\
F(\sum\limits_{i=1}^n t_i v_i)&=F(O)=O && \text{(This is ensured as "output" of a mapping is unique for same "input")}\\
\sum\limits_{i=1}^n t_i F(v_i)=O
\end{aligned}$$
which means if $\sum\limits_{i=1}^n t_i v_i=O$, we must have $\sum\limits_{i=1}^n t_i F(v_i)=O$. Since $F(v_i)$ are linear independent, we obtain that $t_i$ is always equal to $0$, which is another word for $v_i$ are linear independent.\\
It is noteworthy that reversal of this *Lemma* is *NOT* always true as $F(v)=O$ doesn't ensure that $v=O$. In fact, in later subsections, we shall see that $F$ is injective if and only if $\text{Ker } F={O}$.
*** Exercises
**** *Exercise 1*
Only (a), (b), (d), (e), (f), (h) are linear mappings. For (h), it involves */Calculus/*.
**** *Exercise 2*
$T(O)=T[v+(-v)]=T(v)+T(-v)=T(v)-T(v)=O$
**** *Exercise 3*
$T(u+v)=T(u)+T(v)=w+O=w$
**** *Exercise 4*
\\
Let the set of elements $v\in V$ satisfying $T(v)=w$ be $S$. We contend that $\forall v\in S, \exists u\in U:v=u+v_0$.\\
/Proof./ let $u=v-v_0$. $F(u)=F(v-v_0)=F(v)-F(v_0)=O$. Thus, $v\in U$. This means $S\subset (v_0+U)$.\\
Conversely, we contend that $\forall u\in U$, we have $(v_0+u)\in S$.\\
/Proof./ $T(v_0+u)=T(v_0)+T(u)=w+O=w$. This means $(v_0+U)\subset S$.\\
Thus, $S=(v_0+U)$.
**** *Exercise 5*
As *Exercise 2* said, $T(O)=T(v-v)=T(v)+T(-v)=O \Rightarrow T(-v)=-T(v)$.
**** *Exercise 6*
\\
Firstly, $F(v_1+v_2)=(f(v_1)+f(v_2),g(v_1)+g(v_2))=(f(v_1),g(v_1))+(f(v_2),g(v_2))=F(v_1)+F(v_2)$ \\
Secondly, $F(cv)=(cf(v),cg(v))=c(f(v),g(v))=cF(v)$
**** *Exercise 7*
1. Prove $(u_1+u_2)\in U$. We have $F(u_1+u_2)=F(u_1)+F(u_2)=O+O=O$.
2. Prove $cu\in U$. We have $F(cu)=cF(u)=O$.
**** *Exercise 8*
Mapping 8 is linear, others are not.
**** *Exercise 9*
\\
By definition later introduced in $\S 5$, it is line segment between $F(v)$ and $F(v+w)$.\\
If $F(w)\not = O$, then it is a line segment. If $F(w)=O$, then it is a point.
**** *Exercise 10*
By definition, it is a parallelogram.
**** *Exercise 11*
Note that $E_1,E_2$ are standard generators.
Since $S$ is a set of points that can be written in the form $t_1E_1+t_2E_2$ where $0\leq t_1\leq 1$ and $0\leq t_2\leq 1$. Thus, $F(t_1E_1+t_2E_2)=t_1F(E_1)+t_2F(E_2)$ where $0\leq t_1\leq 1$ and $0\leq t_2\leq 1$. Hence, prove the statement.
**** *Exercise 12*
We know $3E_1$ and $E_2$ are also linear independent. So are $3F(E_1)$ and $F(E_2)$. Thus, adopting similar reasoning in *Exercise 11*, we prove statement.
**** *Exercise 13*
It is a parallelogram generated by $5A$ and $2B$.
**** *Exercise 14*
$T_u(v_1+v_2)=v_1+v_2+u=T_u(v_1)+T_u(v_2)=v_1+v_2+2u$. Thus, we have $2u=u$ and $u=O$.
**** *Exercise 15*
It is shown in [[lemma_3.2.1][Lemma]].
**** *Exercise 16*
\\
If $v\in W$, simply let $c=0$ and $w=v$.\\
If $v\not \in W$, let $c=\frac{F(v)}{F(v_0)}$ and $w=v-cv_0$. We then contend that $w\in W$. Since $F(w)=F(v-cv_0)=F(v)-cF(v_0)=0$, we conclude $w\in W$. Thus, concludes.
**** *Exercise 17*
\\
We see that $F(w_1+w_2)=F(w_1)+F(w_2)=0+0=0$. Also, $F(cw)=cF(w)=0$. And since $F$ is linear and as we proved before, $F(O)=O$ and $O\in W$. Thus, $W$ is a subspace of $V$.\\
We know by *Exercise 16* that $\{v_0,v_1,\cdots , v_n\}$ generates $V$. We contend then that they are linear independent. It is by definition that $\{v_1,\cdots , v_n\}$ are linear independent. Since $v_0\not \in W$, we see that $v_0$ cannot be expressed by linear combination of $\{v_0,v_1,\cdots , v_n\}$, [[linear-independence-equivalence][thereby]] $v_0$ is linear independent from others. Thus, they are linear independent. Then, by definition, this set is a basis of $V$.
**** *Exercise 18*
1. $(-1,-1)$
2. $(-\frac{2}{3},1)$
3. $(-2,-1)$
**** *Exercise 19*
1. $(4,5)$
2. $(\frac{11}{3},-3)$
3. $(4,2)$
** $\S$ 3
*** Notes
Another part for *Theorem 3.3*. If $\text{Im } L=W$, then $\dim \text{Im } L=\dim W$ and $\dim \text{Ker }L=0$. Thus, $\text{Ker } L=\{O\}$.
*** Exercises
**** *Exercise 1*
\\
We know $\dim \mathbb{R}^{n}=n$. According to rank-nullity law, we see that $\dim \mathbb{R}^2=\dim \text{Ker }F+\dim \text{Im }F$. Thus, $2=\dim \text{Ker }F+n$ and $2-n=\dim \text{Ker }F$ Since $n,\dim \text{Ker }F\geq 0$, we see that $0\leq n\leq 2$.
1. $n=2$, then $\dim \text{Ker }F=0$ and $\text{Ker }F=\{O\}$. This means that $t_1F(A)+t_2F(B)=O \Rightarrow F(t_1A+t_2B)=O \Rightarrow t_1A+t_2B=O \Rightarrow t_1=t_2=0$.
2. $n=1$, then $\dim \text{Im }F=1$.
3. $n=0$, then $\dim \text{Im }F=0$ and $\text{Im }F=\{O\}$.
**** *Exercise 2*
\\
We know then that $\dim \text{Ker }F\not =0$. Since $2-\dim \text{Ker } F=\dim \text{Im }F$, we see that $\dim \text{Im }F=0$ or $1$. This concludes our prove.
**** *Exercise 3*
Consider $L:\mathbb{R}^4\rightarrow \mathbb{R}^2$ such that $L(x_1,x_2,x_3,x_4)=L(x_1+2x_2,x_3-15x_4)$. According to rank-nullity theorem and since $\text{Ker }L=W$, we see that $\dim \mathbb{R}^4=\dim W+\dim \mathbb{R}^{2}$. Thus $\dim W=2$.
**** *Exercise 4*
We contend that there exists such a $u$. $\forall X$, $L(X-v_0)=L(X)-L(v_0)=O$. This means if we let $u=X-v_0$, then $u\in \text{Ker }L$.
**** *Exercise 5-9*
/*Calculus Involved, not done now.*/
**** *Exercise 10*
\\
1. Let such a subspace be $W$. Consider $L:\mathbb{R}^n\rightarrow \mathbb{R}$ such that $L(X)=\sum\limits_{i=1}^n x_i$. According to rank-nullity theorem and since $\text{Ker }L=W$, we see that $\dim \mathbb{R}^n=\dim W+\dim \mathbb{R}$. Thus $\dim W=n-1$.
2. Let such a subspace be $W$. Consider $tr:\text{Mat}_{n\times n}(\mathbb{R})\rightarrow \mathbb{R}$ such that $tr(A)=\sum\limits_{i=1}^n a_ii$. According to rank-nullity theorem and since $\text{Ker }tr=W$, we see that $\dim \text{Mat}_{n\times n}(\mathbb{R})=\dim W+\dim \mathbb{R}$. Thus $\dim W=n^2-1$.
**** *Exercise 11*
1. We have $tr(A+B)=\sum\limits_{i=1}^n(a_{ii}+b_{ii})=\sum\limits_{i=1}^n a_{ii}+\sum\limits_{i=1}^n b_{ii}=tr(A)+tr(B)$ and $tr(cA)=\sum\limits_{i=1}^n(ca_{ii})=c\sum\limits_{i=1}^n a_{ii}=c\cdot tr(A)$. This concludes linearity for $tr$.
2. $$\begin{aligned}
   tr(AB)&=\sum\limits_{i=1}^n A_iB^i\\
   &=\sum\limits_{i,j=1}^n a_{ij}b_{ji}\\
   &=\sum\limits_{i,j=1}^n b_{ji}a_{ij}\\
   &=\sum\limits_{i=1}^{n} B_iA^i\\
   &=tr(BA)\\
   \end{aligned}$$
3. Since we know $tr(AB)=tr(BA)$, we have $tr[(B^{-1}A)B]=tr[B(B^{-1}A)]=tr[(BB^{-1})A]=tr(I_nA)=tr(A)$
4. Firstly, $\langle A,B \rangle = tr(AB)=tr(BA)=\langle B,A \rangle$. Secondly, $\langle A,B+C \rangle=tr[A(B+C)]=tr[AB+AC]=tr(AB)+tr(AC)=\langle A,B\rangle+\langle A,C\rangle$. Thirdly, $c\langle A,B \rangle=c\sum\limits_{i=1}^n A_iB^i=\sum\limits_{i=1}^n (cA_i)B^i=\langle cA,B \rangle$.
5. $tr(AB-BA)=tr(AB)-tr(BA)=0$. Since $tr(I_n)=n$, this could not be possible.
**** *Exercise 12*
$\dim S=\frac{n(n+1)}{2}$. Bases are trivial, skipped.
**** *Exercise 13*
$tr(AA)=\sum\limits_{i,j=1}^n a_{ij}a_{ji}=\sum\limits_{i,j=1}^n (a_{ij})^2\geq 0$
